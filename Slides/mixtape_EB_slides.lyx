#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass beamer
\begin_preamble
%\usetheme{Frankfurt}
%\setbeamertemplate{footline}{Chris Walters (MIT)}

\setbeamertemplate{navigation symbols}{}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{graphics}
\usepackage{multirow}
\usepackage{xkeyval}
\usepackage{verbatim}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{pdfpages}
\usepackage{xcolor}
\newsavebox\MBox
\newcommand{\Cline}[2][red]{{\sbox\MBox{$#2$}%
  \rlap{\usebox\MBox}\color{#1}\rule[-1.2\dp\MBox]{\wd\MBox}{0.75pt}}}
\newcommand{\Perp}{\perp \! \! \! \perp}

\setbeamertemplate{footline}
{
  \leavevmode%
  \hbox{%
  	\begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1.0ex,center]{author in head/foot}%
    	\usebeamerfont{author in head/foot}%
  		Chris Walters\hspace{1em}\beamer@ifempty{\insertshortinstitute}{}{(UC Berkeley)}
  	\end{beamercolorbox}%
  	\begin{beamercolorbox}[wd=.666666\paperwidth,ht=2.5ex,dp=1.0ex,center]{title in head/foot}%
  	  \usebeamerfont{title in head/foot} {Empirical Bayes Mixtape Session}\hspace{1.5cm}
  	\end{beamercolorbox}
  }%
  \vskip0pt%
}

% Mixtape Sessions Colors
\usepackage{xcolor}
\definecolor{zinc50}{HTML}{fafafa}
\definecolor{zinc100}{HTML}{f4f4f5}
\definecolor{zinc200}{HTML}{e4e4e7}
\definecolor{zinc300}{HTML}{d4d4d8}
\definecolor{zinc400}{HTML}{a1a1aa}
\definecolor{zinc500}{HTML}{71717a}
\definecolor{zinc600}{HTML}{52525b}
\definecolor{zinc700}{HTML}{3f3f46}
\definecolor{zinc800}{HTML}{27272a}
\definecolor{zinc900}{HTML}{18181b}
\definecolor{zinc950}{HTML}{09090b}
\definecolor{picton-blue}{HTML}{00b7ff}
\definecolor{violet-red}{HTML}{ff3881}
\definecolor{sun}{HTML}{ffaf18}
\definecolor{electric-violet}{HTML}{871EFF}
\definecolor{accent}{HTML}{182848}

% Beamer fonts
\setbeamercolor{frametitle}{bg = white, fg = accent}
\setbeamercolor{framesubtitle}{bg = white, fg = accent}
\setbeamerfont{framesubtitle}{size = \small, shape = \itshape}

\setbeamercolor{author in head/foot}{bg = white, fg = accent}
\setbeamercolor{title in head/foot}{bg = white, fg = electric-violet}

% Bullet points
%% enumerate item color
\setbeamercolor{enumerate item}{fg = zinc600}
\setbeamerfont{enumerate item}{size = \small}
\setbeamertemplate{enumerate item}{\insertenumlabel.}

%% itemize
\setbeamercolor{itemize item}{fg = zinc600}
% \setbeamerfont{itemize item}{size = \small}
% \setbeamertemplate{itemize item}[circle]

%% right arrow for subitems
\setbeamercolor{itemize subitem}{fg = zinc600}
% \setbeamerfont{itemize subitem}{size = \small}
% \setbeamertemplate{itemize subitem}{$\rightarrow$}

\setbeamercolor{itemize subsubitem}{fg = zinc600}
% \setbeamerfont{itemize subsubitem}{size = \small}
% \setbeamertemplate{itemize subsubitem}[square]

% Change text margins
\setbeamersize{text margin left = 20pt, text margin right = 20pt}  

% \imageframe{img_name}
% from https://github.com/mattzinc900well/cousteau
\usepackage[beamer,customcolors]{hf-tikz}
\usetikzlibrary{calc,fit,shapes.misc}
\newcommand{\imageframe}[1]{%
    \begin{frame}[plain]
        \begin{tikzpicture}[remember picture, overlay]
            \node[at = (current page.center), xshift = 0cm] (cover) {%
                \includegraphics[keepaspectratio, width=\paperwidth, height=\paperheight]{#1}
            };
        \end{tikzpicture}
    \end{frame}%
}
\end_preamble
\options navy,handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 0
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\fontcolor #000000
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 1
\tocdepth 1
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout PlainFrame

\end_layout

\begin_layout PlainFrame

\end_layout

\begin_layout PlainFrame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}[remember picture, overlay]
\end_layout

\begin_layout Plain Layout

  
\backslash
node[at = (current page.center), xshift = 0cm] (cover) {%
\end_layout

\begin_layout Plain Layout

    
\backslash
includegraphics[keepaspectratio, width=
\backslash
paperwidth, height=
\backslash
paperheight]{cover.png}%
\end_layout

\begin_layout Plain Layout

  };
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Empirical Bayes and Large-Scale Inference
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size small
Economists are increasingly drilling down to study heterogeneity in fine-grained
, unit-specific parameters
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size small
Returns to a year of education 
\begin_inset Formula $\implies$
\end_inset

 Returns to college selectivity 
\begin_inset Formula $\implies$
\end_inset

 Returns to specific colleges 
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
scriptsize 
\backslash
textcolor{red!75!green!50!blue!25!gray}{(Card, 1999; Dale and Krueger, 2002,
 2014; Mountjoy and Hickman, 2021)}}
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size small
Industry wage premia 
\begin_inset Formula $\implies$
\end_inset

 Firm-specific wage premia 
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
scriptsize 
\backslash
textcolor{red!75!green!50!blue!25!gray}{(Krueger and Summers, 1988; Abowd
 et al., 1999; Card et al., 2018)}}
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size small
Effects of neighborhood characteristics 
\begin_inset Formula $\implies$
\end_inset

 Effects of specific neighborhoods 
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
scriptsize 
\backslash
textcolor{red!75!green!50!blue!25!gray}{(Kling et al., 2007; Chetty and Hendren,
 2018; Chetty et al., 2018)}}
\end_layout

\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size small
\begin_inset Quotes eld
\end_inset

Value-added
\begin_inset Quotes erd
\end_inset

 of individual teachers, schools, doctors, medical centers, managers, police
 officers, judges 
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
scriptsize 
\backslash
textcolor{red!75!green!50!blue!25!gray}{(Chetty et al., 2014; Angrist et
 al., 2017;  Chan et al., 2022; Einav et al., 2022; Fenizia, 2022; Goncalves
 and Mello, 2023; Frandsen et al., 2023)}}
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Empirical Bayes and Large-Scale Inference
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In settings with many unit-specific parameters,
\series bold
 empirical Bayes (EB) 
\series default
methods are useful for several purposes
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Learning about the 
\emph on
distribution
\emph default
 of parameters across units
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Improving estimates for individual units (
\begin_inset Quotes eld
\end_inset

borrowing strength
\begin_inset Quotes erd
\end_inset

)
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Making decisions (Policy: what to do? Scientific: what to report?)
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
The goals of this session are to familiarize students with EB methods and
 provide tools to apply them in practice
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Course Outline
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Part 1: Empirical Bayes basics
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Conceptual framework and empirical Bayes recipe
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Linear shrinkage; James/Stein theorem
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Uses of EB: combining estimators, EB and regression, EB decision rules,
 individualized treatment recommendations
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Comparisons to other methods: Machine learning, full B vs.
 EB
\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
Part 2: Empirical Bayes extensions
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Flexible variance estimation
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Precision-dependence
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Non-parametric deconvolution and shrinkage
\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
Part 3: Large-scale inference
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Empirical Bayes approaches to False Discovery Rate control
\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
Each part will include a coding lab to illustrate the methods, along with
 live-coding of solutions
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Empirical Bayes Basics
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Conceptual Framework
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
We'll start with a basic conceptual framework, illustrated through a running
 example
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Let 
\begin_inset Formula $j\in\{1...J\}$
\end_inset

 index groups (e.g.
 schools), and let 
\begin_inset Formula $i\in\{1...N\}$
\end_inset

 index individuals within groups (e.g.
 students)
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
We observe outcomes 
\begin_inset Formula $Y_{ij}$
\end_inset

 for each individual 
\begin_inset Formula $i$
\end_inset

 in each group 
\begin_inset Formula $j$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
\begin_inset Formula $\theta_{j}$
\end_inset

 is an unknown parameter for group 
\begin_inset Formula $j$
\end_inset

 that determines the distribution of 
\begin_inset Formula $Y_{ij}$
\end_inset

 (e.g.
 the effect of school 
\begin_inset Formula $j$
\end_inset

)
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
For example, suppose test scores are normally distributed and homoskedastic,
 with a school-specific mean:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $Y_{ij}=\theta_{j}+\epsilon_{ij},\ \epsilon_{ij}|\theta_{j}\sim N(0,\sigma_{\epsilon}^{2})$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Estimating 
\begin_inset Formula $\theta_{j}$
\end_inset


\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
We start by using the data for group 
\begin_inset Formula $j$
\end_inset

 to form an estimate 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 of 
\begin_inset Formula $\theta_{j}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
In our school effects example, the natural estimator is the sample mean
 for each school:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\theta}_{j}=\dfrac{1}{N}{\displaystyle \sum_{i=1}^{N}Y_{ij}}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
The 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

's are unbiased, noisy estimates of the unknown 
\begin_inset Formula $\theta_{j}$
\end_inset

's:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\theta}_{j}|\theta_{j}\sim N\left(\theta_{j},s_{j}^{2}\right)$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
(Squared) standard errors 
\begin_inset Formula $s_{j}^{2}\equiv Var\left(\hat{\theta}_{j}|\theta_{j}\right)$
\end_inset

 quantify the noise in 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
With equal group sizes and homoskedastic errors, 
\begin_inset Formula $s_{j}^{2}=\sigma_{\epsilon}^{2}/N\ \forall j$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Introducing 
\begin_inset Formula $G$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Next, we posit a distribution of the parameters 
\begin_inset Formula $\theta_{j}$
\end_inset

 across groups:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\theta_{j}\sim G,\ j=1,....,J$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
The 
\series bold
mixing distribution
\series default
 
\begin_inset Formula $G$
\end_inset

 is a key object in the EB framework
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
\begin_inset Formula $G$
\end_inset

 is an objective feature of the world, not a subjective prior
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
\begin_inset Formula $G$
\end_inset

 answers questions about variation in parameters
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
How much does school quality vary? 
\begin_inset Formula $\sigma_{\theta}^{2}=\int(\theta-\mu_{\theta})^{2}dG(\theta)$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
What's the difference between 75th and 25th percentile schools? 
\begin_inset Formula $G^{-1}(0.75)-G^{-1}(0.25)$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
EB 
\series bold
deconvolution
\series default
: Use noisy estimates 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 along with standard errors 
\begin_inset Formula $s_{j}$
\end_inset

 to compute an estimate 
\begin_inset Formula $\hat{G}$
\end_inset

 of 
\begin_inset Formula $G$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Philosophy of 
\begin_inset Formula $G$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
What does it mean to treat the 
\begin_inset Formula $\theta_{j}$
\end_inset

 parameters as random draws from a distribution 
\begin_inset Formula $G$
\end_inset

?
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
\begin_inset Quotes eld
\end_inset

Fixed effects
\begin_inset Quotes erd
\end_inset

 perspective: There are 
\begin_inset Formula $J$
\end_inset

 units, with fixed but unknown parameters 
\begin_inset Formula $\{\theta_{j}\}_{j=1}^{J}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
One (unsatisfying) answer: observed units are sampled from some larger superpopu
lation 
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
\begin_inset Quotes eld
\end_inset

Random effects
\begin_inset Quotes erd
\end_inset

 perspective can be motivated by analyst's objectives
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Even with finite population of units, we can ask how the 
\begin_inset Formula $\theta_{j}$
\end_inset

's are distributed in this population
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
If our loss function cares about average performance across units, it's
 valuable to incorporate distributional information into estimates for individua
ls
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Think of continuous/
\emph on
iid
\emph default
 models for 
\begin_inset Formula $G$
\end_inset

 as parsimonious approximation
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Random vs.
 fixed effects is 
\emph on
not 
\emph default
about correlation of 
\begin_inset Formula $\theta_{j}$
\end_inset

's with observables (c.f.
 
\begin_inset Quotes eld
\end_inset

random effects
\begin_inset Quotes erd
\end_inset

 vs.
 
\begin_inset Quotes eld
\end_inset

correlated random effects
\begin_inset Quotes erd
\end_inset

)
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Normal/Normal Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
In school effects example, suppose 
\begin_inset Formula $G$
\end_inset

 is a normal distribution and independent of 
\begin_inset Formula $s_{j}$
\end_inset

:
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Then we have the hierarchical model
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\theta}_{j}|\theta_{j},s_{j}\sim N(\theta_{j},s_{j}^{2})$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\theta_{j}|s_{j}\sim N(\mu_{\theta},\sigma_{\theta}^{2})$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
\size footnotesize
Hyperparameters
\series default
 
\begin_inset Formula $\mu_{\theta}$
\end_inset

 and 
\begin_inset Formula $\sigma_{\theta}^{2}$
\end_inset

 summarize the distribution of lower-level parameters
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
With this model for 
\begin_inset Formula $G$
\end_inset

, deconvolution just requires estimating these two hyperparameters
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Estimating Hyperparameters
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Simple estimators for hyperparameters:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\mu}_{\theta}=\dfrac{1}{J}{\displaystyle \sum_{j=1}^{J}\hat{\theta}_{j}}$
\end_inset

,
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\sigma}_{\theta}^{2}=\dfrac{1}{J}{\displaystyle \sum_{j=1}^{J}}\left[(\hat{\theta}_{j}-\hat{\mu}_{\theta})^{2}-s_{j}^{2}\right]$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Subtracting the average squared standard error 
\begin_inset Formula $s_{j}^{2}$
\end_inset

 is a 
\series bold
bias-correction 
\series default
accounting for excess variance in the 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

's due to sampling error
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
\begin_inset Formula $\hat{\sigma}_{\theta}^{2}>0$
\end_inset

 
\begin_inset Formula $\implies$
\end_inset

 
\series bold
overdispersion
\series default
 beyond what we'd expect from noise
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
Other options: precision-weighting; maximum likelihood estimation (MLE)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Shrinkage
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
In normal/normal model, posterior mean for 
\begin_inset Formula $\theta_{j}$
\end_inset

 given 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 is:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\theta_{j}^{*}\equiv E[\theta_{j}|\hat{\theta}_{j}]=\left(\dfrac{\sigma_{\theta}^{2}}{\sigma_{\theta}^{2}+s_{j}^{2}}\right)\hat{\theta}_{j}+\left(\dfrac{s_{j}^{2}}{\sigma_{\theta}^{2}+s_{j}^{2}}\right)\mu_{\theta}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Posterior mean 
\series bold
shrinks
\series default
 noisy estimate 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 toward prior mean based on signal-to-noise ratio
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Linear shrinkage formula is motivated by normality, but has good properties
 regardless of the form of 
\begin_inset Formula $G$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Coincides with fitted value from OLS regression of 
\begin_inset Formula $\theta_{j}$
\end_inset

 on 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 
\begin_inset Formula $\implies$
\end_inset

 inherits usual OLS 
\begin_inset Quotes eld
\end_inset

best linear predictor
\begin_inset Quotes erd
\end_inset

 interpretation
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EB Posterior Means
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Putting the 
\begin_inset Quotes eld
\end_inset

E
\begin_inset Quotes erd
\end_inset

 in 
\begin_inset Quotes eld
\end_inset

EB
\begin_inset Quotes erd
\end_inset

 – Empirical Bayes posterior mean 
\begin_inset Formula $\hat{\theta}_{j}^{*}$
\end_inset

 plugs in estimated hyperparameters:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\theta}_{j}^{*}=\left(\dfrac{\hat{\sigma}_{\theta}^{2}}{\hat{\sigma}_{\theta}^{2}+s_{j}^{2}}\right)\hat{\theta}_{j}+\left(\dfrac{s_{j}^{2}}{\hat{\sigma}_{\theta}^{2}+s_{j}^{2}}\right)\hat{\mu}_{\theta}$
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
EB posterior shrinks estimate for school 
\begin_inset Formula $j$
\end_inset

 using hyperparameters estimated with the larger pool of schools
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Reflects general EB approach: Use deconvolution estimate 
\begin_inset Formula $\hat{G}$
\end_inset

 as prior when forming posteriors for individual units
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
\begin_inset Quotes eld
\end_inset

Borrowing strength from the ensemble
\begin_inset Quotes erd
\end_inset

 (Efron and Morris, 1973; Morris, 1983)
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
\begin_inset Quotes eld
\end_inset

Indirect evidence
\begin_inset Quotes erd
\end_inset

 (Efron, 2010)
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
\begin_inset Quotes eld
\end_inset

Learning from the experience of others
\begin_inset Quotes erd
\end_inset

 (Efron, 2012)
\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Summary: A Three-step EB Recipe
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
\size small
Estimation: 
\series default
Estimate parameter and standard error for each unit 
\begin_inset Formula $\implies\{\hat{\theta}_{j},s_{j}\}_{j=1}^{J}$
\end_inset


\series bold

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Enumerate

\series bold
\size small
Deconvolution: 
\series default
Use 
\begin_inset Formula $\{\hat{\theta}_{j},s_{j}\}_{j=1}^{J}$
\end_inset

 to estimate mixing distribution 
\begin_inset Formula $\implies\hat{G}$
\end_inset


\series bold

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Enumerate

\series bold
\size small
Shrinkage: 
\series default
Treating 
\begin_inset Formula $\hat{G}$
\end_inset

 as prior, update with 
\begin_inset Formula $(\hat{\theta}_{j},s_{j})$
\end_inset

 to form posterior 
\begin_inset Formula $\implies\{\hat{\theta}_{j}^{*}\}_{j=1}^{J}$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
When to Shrink?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Should we prefer the shrunk posterior mean to the unbiased estimate 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

? It depends on our goals
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Continue with the normal/normal model:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\theta}_{j}|\theta_{j},s_{j}\sim N(\theta_{j},s_{j}^{2}),\ \ \theta_{j}|s_{j}\sim N(\mu_{\theta},\sigma_{\theta}^{2})$
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Conditional on the latent parameter for school 
\begin_inset Formula $j$
\end_inset

, mean squared error (MSE) for the two estimators is:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $E\left[(\hat{\theta}_{j}-\theta_{j})^{2}|\theta_{j},s_{j}\right]=s_{j}^{2}$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $E\left[(\hat{\theta}_{j}-\theta_{j})^{2}|\theta_{j},s_{j}\right]=\left(\tfrac{\sigma_{\theta}^{2}}{\sigma_{\theta}^{2}+s_{j}^{2}}\right)^{2}s_{j}^{2}+\left(\tfrac{s_{j}^{2}}{\sigma_{\theta}^{2}+s_{j}^{2}}\right)^{2}\left(\theta_{j}-\mu_{\theta}\right)^{2}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
If we're only interested in one school (e.g.
 
\begin_inset Formula $\theta_{1}$
\end_inset

), not clear which is better
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Shrinkage reduces variance, but may introduce substantial bias if the school
 is very different from average
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Shrinkage Reduces Aggregate MSE
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Now suppose we're equally interested in all schools
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
In this case the relevant notion of MSE integrates over 
\begin_inset Formula $G$
\end_inset

:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $E\left[(\hat{\theta}_{j}-\theta_{j})^{2}|s_{j}\right]=\int E\left[(\hat{\theta}_{j}-\theta)^{2}|\theta_{j}=\theta,s_{j}\right]dG(\theta)=s_{j}^{2}$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $E\left[(\theta_{j}^{*}-\theta_{j})^{2}|s_{j}\right]=\int E\left[(\theta_{j}^{*}-\theta)^{2}|\theta_{j}=\theta,s_{j}\right]dG(\theta)=\left(\tfrac{\sigma_{\theta}^{2}}{\sigma_{\theta}^{2}+s_{j}^{2}}\right)s_{j}^{2}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Linear shrinkage estimate is superior if we want an estimator that performs
 well 
\emph on
on average across schools
\emph default

\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Related to classic 
\series bold
James/Stein
\series default
 (1961) result: sample mean is inadmissible under squared loss and dominated
 by shrinkage-based estimators when estimating at least three parameters
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The James/Stein Theorem 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Dispense with random effects framework for a moment, and suppose we're intereste
d in 
\begin_inset Formula $J\geq3$
\end_inset

 parameters 
\begin_inset Formula $(\theta_{1},...,\theta_{J})^{\prime}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Let 
\begin_inset Formula $\hat{\theta}_{j}|\theta_{j}\sim N(\theta_{j},1)$
\end_inset

, independent across 
\begin_inset Formula $j$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
We're interested in choosing an estimator with low total MSE across all
 
\begin_inset Formula $J$
\end_inset

 parameters
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
MSE of unbiased estimates:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $MSE_{\hat{\theta}}={\displaystyle \sum_{j=1}^{J}}E\left[(\hat{\theta}_{j}-\theta_{j})^{2}|\theta_{j}\right]=J$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The James/Stein Theorem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Consider an alternative shrinkage estimator of the form: 
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\hat{\theta}_{j}^{JS}=\left(1-\dfrac{(J-2)}{\sum_{k=1}^{J}(\hat{\theta}_{k}-m)^{2}}\right)\hat{\theta}_{j}+\left(\dfrac{(J-2)}{\sum_{k=1}^{J}(\hat{\theta}_{k}-m)^{2}}\right)m$
\end_inset


\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
This estimator shrinks 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 toward a constant 
\begin_inset Formula $m$
\end_inset

, using sum of squares to choose the shrinkage factor
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
\begin_inset Formula $\hat{\theta}_{j}^{JS}$
\end_inset

 dominates the unbiased estimator 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 on MSE grounds, as shown by the 
\series bold
James/Stein (1961) Theorem
\series default
: 
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $MSE_{\hat{\theta}^{JS}}=E\left[\sum_{j}\left(\hat{\theta}_{j}^{JS}-\theta_{j}\right)^{2}|\theta_{j}\right]$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\leq J-\dfrac{(J-2)^{2}}{J-2+\sum_{j=1}^{J}(\theta_{j}-m)^{2}}$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $<J$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $=MSE_{\hat{\theta}}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
James/Stein Discussion
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
JS shrinkage emerges naturally as EB procedure based on normal 
\begin_inset Formula $G$
\end_inset

 with known mean 
\begin_inset Formula $m$
\end_inset

 and unknown variance
\begin_inset VSpace medskip
\end_inset


\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
If 
\begin_inset Formula $\theta_{j}\sim N(m,\sigma_{\theta}^{2})$
\end_inset

, then 
\begin_inset Formula $(J-2)/[\sum_{j}(\hat{\theta}_{j}-m)^{2}]$
\end_inset

 is an unbiased estimate of shrinkage factor 
\begin_inset Formula $1/(1+\sigma_{\theta}^{2})$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
But MSE improvement does not require random effects/Bayesian framework –
 holds for any configuration of 
\begin_inset Formula $\theta_{j}$
\end_inset

's and any 
\begin_inset Formula $m$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Result generalizes to using estimated standard error 
\begin_inset Formula $\neq1$
\end_inset

 and estimated prior mean
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
We can often think of mixing distribution 
\begin_inset Formula $G$
\end_inset

 as a device to motivate procedures with desireable frequentist properties
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Note that JS result requires us to care equally about each of the 
\begin_inset Formula $J\geq3$
\end_inset

 parameters – no guarantee of improvement for any particular 
\begin_inset Formula $\theta_{j}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Should we jointly shrink estimates of the age of the universe, elasticity
 of labor supply, and efficacy of Covid vaccine?
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame

\size footnotesize
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Unbiased Estimates vs.
 Shrunk Posteriors vs.
 True Effects
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
It's important to keep in mind the distinction between unbiased estimates
 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

, linear shrinkage estimates 
\begin_inset Formula $\hat{\theta}_{j}^{*}$
\end_inset

, and true latent parameters 
\begin_inset Formula $\theta_{j}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
If we're interested in understanding variation in the true 
\begin_inset Formula $\theta_{j}$
\end_inset

's, the variance of unbiased estimates 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 is
\emph on
 too big:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $Var(\hat{\theta}_{j})=\sigma_{\theta}^{2}+s_{j}^{2}$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\ \ \ \ >\sigma_{\theta}^{2}$
\end_inset

.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
On the other hand, the variance of shrunk posteriors 
\begin_inset Formula $\theta_{j}^{*}$
\end_inset

 is 
\emph on
too small:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $Var\left(\left(\tfrac{\sigma_{\theta}^{2}}{\sigma_{\theta}^{2}+s_{j}^{2}}\right)\hat{\theta}_{j}+\left(\tfrac{s_{j}^{2}}{\sigma_{\theta}^{2}+s_{j}^{2}}\right)\mu_{\theta}\right)=\left(\dfrac{\sigma_{\theta}^{2}}{\sigma_{\theta}^{2}+s_{j}^{2}}\right)\sigma_{\theta}^{2}$
\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $<\sigma_{\theta}^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize

\size scriptsize
Deconvolution estimate 
\begin_inset Formula $\hat{\sigma}_{\theta}^{2}$
\end_inset

 is the 
\begin_inset Quotes eld
\end_inset

goldilocks variance
\begin_inset Quotes erd
\end_inset

 that gets it just right – consistent estimate of 
\begin_inset Formula $\sigma_{\theta}^{2}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
More generally, to summarize features of the distribution of 
\begin_inset Formula $\theta_{j}$
\end_inset

's, we should use an estimate of 
\begin_inset Formula $G$
\end_inset

 rather than the distribution of 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

's or 
\begin_inset Formula $\theta_{j}^{*}$
\end_inset

's
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EB Application: School Value-Added
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Next, illustrate/extend what we've learned in an application to estimating
 school value-added in Boston (Angrist, Hull, Pathak and Walters 2017)
\begin_inset VSpace medskip
\end_inset


\size scriptsize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Suppose each student attends one of 
\begin_inset Formula $j$
\end_inset

 schools, and let 
\begin_inset Formula $Y_{i}(j)$
\end_inset

 denote student i's potential academic achievement if s/he attends school
 
\begin_inset Formula $j$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Simple additive model for potential outcomes:
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $Y_{i}(j)=\theta_{j}+a_{i}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
Here 
\begin_inset Formula $\theta_{j}$
\end_inset

 is the causal 
\series bold
value-added 
\series default
of school 
\begin_inset Formula $j$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
\begin_inset Formula $a_{i}$
\end_inset

 represents unobserved student heterogeneity (family background, ability,
 etc.).
 Normalize 
\begin_inset Formula $E[a_{i}]=0$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Constant effects model: 
\begin_inset Formula $\theta_{j}-\theta_{k}$
\end_inset

 is the effect of moving any student from school 
\begin_inset Formula $k$
\end_inset

 to school 
\begin_inset Formula $j$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Questions About Schools
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Several possible questions of interest in this setting
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Might be interested in the value-added of a particular school, e.g.
 
\begin_inset Formula $\theta_{1}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
How good is my neighborhood school?
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
Might be interested in features of the 
\emph on
distribution
\emph default
 of 
\begin_inset Formula $\theta_{j}$
\end_inset

's across schools
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
How much does school quality vary?
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
Might be interested in making a decision that depends on the 
\begin_inset Formula $\theta_{j}$
\end_inset

's
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Which school should my child attend? Which school(s) should be closed or
 expanded?
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
EB methods are useful for answering each of these questions
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
VAM Regression
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Letting 
\begin_inset Formula $D_{ij}$
\end_inset

 indicate attendance at 
\begin_inset Formula $j$
\end_inset

, observed outcome is:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $Y_{i}=\sum_{j}\theta_{j}D_{ij}+a_{i}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Project 
\begin_inset Formula $a_{i}$
\end_inset

 on a vector of covariates 
\begin_inset Formula $X_{i}$
\end_inset

 (e.g.
 demographics and lagged achievement):
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $Y_{i}=\sum_{j}\theta_{j}D_{ij}+X_{i}^{\prime}\beta+\epsilon_{i}$
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Here 
\begin_inset Formula $E[X_{i}\epsilon_{i}]=0$
\end_inset

 by definition of 
\begin_inset Formula $\beta$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Suppose we have selection-on-observables: additive control for 
\begin_inset Formula $X_{i}$
\end_inset

 captures all selection bias, so 
\begin_inset Formula $E[D_{ij}\epsilon_{i}]=0\ \forall j$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Then ordinary least squares (OLS) regression recovers the parameters of
 this value-added model (VAM)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Empirical Bayes for School Value-Added
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
OLS VAM estimation yields estimates 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 and standard errors 
\begin_inset Formula $s_{j}$
\end_inset

 (EB Step 1)
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Assume 
\begin_inset Formula $\hat{\theta}_{j}|\theta_{j},s_{j}\sim N(\theta_{j},s_{j}^{2})$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Think of this as an asymptotic approximation with a growing number of students
 per school
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
Second level of the hierarchy posits a mixing distribution for value-added:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\theta_{j}|s_{j}\sim N(\mu_{\theta},\sigma_{\theta}^{2})$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
We can then apply the rest of the EB recipe
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Step 2: Use 
\begin_inset Formula $(\hat{\theta}_{j},s_{j})$
\end_inset

 to estimate hyperparameters 
\begin_inset Formula $\hat{\mu}_{\theta}$
\end_inset

 and 
\begin_inset Formula $\hat{\sigma}_{\theta}^{2}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Step 3: Form linear shrinkage value-added estimates 
\begin_inset Formula $\hat{\theta}_{j}^{*}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
VAM Standard Deviations for Boston Middle Schools (Sixth Grade Math)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename vam_sd.pdf
	lyxscale 50
	scale 65

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Histogram of Lagged Score VAM Estimates for Boston (Sixth Grade Math, 2014)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename hist_1.png
	lyxscale 50
	scale 40

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Prior Distribution Pooling Sectors
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename hist_2.png
	lyxscale 50
	scale 40

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Posterior Means Pooling Sectors
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename hist_3.png
	lyxscale 50
	scale 40

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Incorporating Covariates
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
It is often natural to build observed covariates into EB estimates
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Learning from the experience of 
\emph on
which
\emph default
 others?
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Mixed/multi-level models: allow 
\begin_inset Formula $\theta_{j}$
\end_inset

's to depend on school-level characteristics (Raudenbush and Bryk, 1986)
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
Model for 
\begin_inset Formula $G$
\end_inset

 conditional on a vector of characteristics 
\begin_inset Formula $C_{j}$
\end_inset

, e.g.
 charter sector indicator:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\theta_{j}|s_{j},C_{j}\sim N\left(C_{j}^{\prime}\mu,\sigma_{r}^{2}\right)$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Estimate 
\begin_inset Formula $\mu$
\end_inset

 from regression of 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 on 
\begin_inset Formula $C_{j}$
\end_inset

; deconvolve residuals 
\begin_inset Formula $\hat{r}_{j}=\hat{\theta}_{j}-C_{j}^{\prime}\hat{\mu}$
\end_inset

 to estimate 
\begin_inset Formula $\sigma_{r}^{2}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Resulting EB posterior shrinks 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 toward estimated linear index:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\theta}_{j}^{*}=\left(\dfrac{\hat{\sigma}_{r}^{2}}{\hat{\sigma}_{r}^{2}+s_{j}^{2}}\right)\hat{\theta}_{j}+\left(\dfrac{s_{j}^{2}}{\hat{\sigma}_{r}^{2}+s_{j}^{2}}\right)C_{j}^{\prime}\hat{\mu}$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Prior with Charter Sector Location Shift
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename hist_4.png
	lyxscale 50
	scale 40

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Posteriors Shrinking Toward Sector Means
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename hist_5.png
	lyxscale 50
	scale 40

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Combining Estimators
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
EB framework extends naturally to cases where we have multiple estimates
 of the same parameter, some possibly biased
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Changing notation, let 
\begin_inset Formula $\hat{\alpha}_{j}$
\end_inset

 denote OLS estimate for school 
\begin_inset Formula $j$
\end_inset

, and suppose selection-on-observables fails, represented by bias parameter
 
\begin_inset Formula $b_{j}$
\end_inset

:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\alpha}_{j}|\theta_{j},b_{j},s_{j\alpha}\sim N\left(\theta_{j}+b_{j},s_{j\alpha}^{2}\right)$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Suppose we also have a noisy but (asymptotically) unbiased estimate 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

, e.g.
 IV estimate from randomized lottery :
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\theta}_{j}|\theta_{j},b_{j},s_{j\theta}\sim N(\theta_{j},s_{j\theta}^{2})$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Suppose a Hausman test rejects OLS = IV.
 Should we throw away OLS?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EB for Bias Correction
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\hat{\alpha}_{j}|\theta_{j},b_{j},s_{j\alpha}\sim N\left(\theta_{j}+b_{j},s_{j\alpha}^{2}\right)$
\end_inset

, 
\begin_inset Formula $\hat{\theta}_{j}|\theta_{j},b_{j},s_{j\theta}\sim N(\theta_{j},s_{j\theta}^{2})$
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
We can use the ensemble 
\begin_inset Formula $\{\hat{\alpha}_{j},\hat{\theta}_{j}\}_{j=1}^{J}$
\end_inset

 to estimate the joint distribution of truth and bias
\begin_inset VSpace medskip
\end_inset


\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
EB 
\begin_inset Quotes eld
\end_inset

hybrid
\begin_inset Quotes erd
\end_inset

 posterior 
\begin_inset Formula $\hat{\theta}_{j}^{*}=E_{\hat{G}}[\theta_{j}|\hat{\theta}_{j},\hat{\alpha}_{j}]$
\end_inset

 trades off bias and variance to minimize MSE
\begin_inset VSpace medskip
\end_inset


\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
In special case with 
\begin_inset Formula $s_{j,\alpha}^{2}\approx0$
\end_inset

 we have
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\hat{\theta}_{j}^{*}=\hat{\tau}_{\theta}\hat{\theta}_{j}+(1-\hat{\tau}_{\theta})\left(\hat{r}_{\alpha}(\hat{\alpha}_{j}-\hat{\mu}_{b})+(1-\hat{r}_{\alpha})\hat{\mu}_{\theta}\right)$
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Here 
\begin_inset Formula $\tau_{\theta}=\tfrac{\sigma_{\theta}^{2}(1-R^{2})}{\sigma_{\theta}^{2}(1-R^{2})+s_{j\theta}^{2}}$
\end_inset

, 
\begin_inset Formula $R^{2}$
\end_inset

 is R-squared from regression of 
\begin_inset Formula $\theta_{j}$
\end_inset

 on 
\begin_inset Formula $\alpha_{j}$
\end_inset

, and 
\begin_inset Formula $r_{\alpha}$
\end_inset

 is slope coefficient from this regression (aka reliability of OLS)
\begin_inset VSpace medskip
\end_inset


\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Angrist et al.
 (2017, forthcoming) generalize to underidentified case; see also Chetty
 and Hendren (2018)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
MSE Improvements from Lottery-based Hybrid Estimates
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename hybrid_mse.pdf
	lyxscale 50
	scale 65

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Shrinkage and Regression
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Suppose we want to know how 
\begin_inset Formula $\theta_{j}$
\end_inset

 varies with a covariate 
\begin_inset Formula $C_{j}$
\end_inset

, e.g., charter status
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
In other words, we'd like to know the regression of 
\begin_inset Formula $\theta_{j}$
\end_inset

 on 
\begin_inset Formula $C_{j}$
\end_inset

, given by 
\begin_inset Formula $Cov(\theta_{j},C_{j})/Var(C_{j})$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Two options:
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate

\size footnotesize
Regress noisy 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 on 
\begin_inset Formula $C_{j}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Enumerate

\size footnotesize
Regress shrunk 
\begin_inset Formula $\hat{\theta}_{j}^{*}$
\end_inset

 on 
\begin_inset Formula $C_{j}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
Which of these approaches recovers the regression of 
\begin_inset Formula $\theta_{j}$
\end_inset

 on 
\begin_inset Formula $C_{j}$
\end_inset

 (if any)?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Shrinkage on the Left Causes Bias
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
If noise 
\begin_inset Formula $\hat{\theta}_{j}-\theta_{j}$
\end_inset

 is independent of 
\begin_inset Formula $C_{j}$
\end_inset

, a regression of 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 on 
\begin_inset Formula $C_{j}$
\end_inset

 gives the right answer:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\dfrac{Cov(\hat{\theta}_{j},C_{j})}{Var(C_{j})}=\dfrac{Cov(\theta_{j},C_{j})}{Var(C_{j})}+\dfrac{Cov(\hat{\theta}_{j}-\theta_{j},C_{j})}{Var(C_{j})}=\dfrac{Cov(\theta_{j},C_{j})}{Var(C_{j})}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
This is a consequence of the fact that classical measurement error on the
 left-hand side of a regression yields no bias
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
In contrast, shrinkage on the left leads to bias.
 With a common shrinkage factor 
\begin_inset Formula $\lambda=\sigma_{\theta}^{2}/(\sigma_{\theta}^{2}+s^{2})$
\end_inset

, we have
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\dfrac{Cov(\theta_{j}^{*},C_{j})}{Var(C_{j})}=\dfrac{Cov(\lambda\hat{\theta}_{j},C_{j})}{Var(C_{j})}=\lambda\dfrac{Cov(\hat{\theta}_{j},C_{j})}{Var(C_{j})}=\lambda\dfrac{Cov(\theta_{j},C_{j})}{Var(C_{j})}$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Shrinkage and Regression
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
What if we wanted to know how some variable 
\begin_inset Formula $W_{j}$
\end_inset

 varies with 
\begin_inset Formula $\theta_{j}$
\end_inset

?
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
In other words, we'd like to know the regression of 
\begin_inset Formula $W_{j}$
\end_inset

 on 
\begin_inset Formula $\theta_{j}$
\end_inset

, given by 
\begin_inset Formula $Cov(W_{j},\theta_{j})/Var(\theta_{j})$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Two options:
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate

\size footnotesize
Regress 
\begin_inset Formula $W_{j}$
\end_inset

 on noisy 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Enumerate

\size footnotesize
Regress 
\begin_inset Formula $W_{j}$
\end_inset

 on shrunk 
\begin_inset Formula $\hat{\theta}_{j}^{*}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
Which of these approaches recovers the regression of 
\begin_inset Formula $W_{j}$
\end_inset

 on 
\begin_inset Formula $\theta_{j}$
\end_inset

 (if any)?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Shrinkage on the Right Fixes Bias
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
When 
\begin_inset Formula $\theta_{j}$
\end_inset

 is on the right, using the unbiased estimate yields attenuation bias:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\dfrac{Cov(W_{j},\hat{\theta}_{j})}{Var(\hat{\theta}_{j})}=\dfrac{Cov(W_{j},\theta_{j})+Cov(W_{j},\hat{\theta}_{j}-\theta_{j})}{Var(\theta_{j})+Var(\hat{\theta}_{j}-\theta_{j})}=\lambda\dfrac{Cov(W_{j},\theta_{j})}{Var(\theta_{j})}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
In contrast, shrinkage of the regressor gives the right answer:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\dfrac{Cov(W_{j},\lambda\hat{\theta}_{j})}{Var(\lambda\hat{\theta}_{j})}=\dfrac{\lambda Cov(W_{j},\hat{\theta}_{j})}{\lambda^{2}Var(\hat{\theta}_{j})}=\dfrac{1}{\lambda}\dfrac{Cov(W_{j},\hat{\theta}_{j})}{Var(\hat{\theta}_{j})}=\dfrac{Cov(W_{j},\theta_{j})}{Var(\theta_{j})}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
This is a version of errors-in-variables regression – use estimated signal-to-no
ise ratio to correct attenuation bias
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Rule of thumb: Put unbiased estimates on the left, and shrunk posteriors
 on the right
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EB Decision Rules
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
EB posterior means deliver estimates with low MSE
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
We often have goals other than minimizing MSE
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Example: Suppose we want to close schools with value-added below a cutoff
 
\begin_inset Formula $c$
\end_inset

 (Gu and Koenker, 2023)
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Loss function for decision 
\begin_inset Formula $\delta_{j}\in\{0,1\}$
\end_inset

:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\mathcal{L}(\theta_{j},\delta_{j})=\delta_{j}1\left\{ \theta_{j}>c\right\} +(1-\delta_{j})1\left\{ \theta_{j}\leq c\right\} \kappa$
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Cost 1 of mistakenly closing high-performing school; cost 
\begin_inset Formula $\kappa$
\end_inset

 of failing to close low-performing school
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Minimizing Risk
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
We do not observe the 
\begin_inset Formula $\theta_{j}$
\end_inset

's, so we must choose a decision rule 
\begin_inset Formula $\delta(\hat{\theta}_{j},s_{j})$
\end_inset

 using noisy estimates and standard errors
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
With 
\begin_inset Formula $J$
\end_inset

 schools, the 
\series bold
risk
\series default
 (expected loss) of decision rule 
\begin_inset Formula $\delta$
\end_inset

 is given by:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\mathcal{R}_{\delta}=E\left[\sum_{j}\mathcal{L}(\theta_{j},\delta(\hat{\theta}_{j},s_{j}))\right]$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $={\displaystyle \sum_{j}\int}{\displaystyle \int\mathcal{L}(\theta,\delta(\hat{\theta},s_{j}))\dfrac{1}{s_{j}}\phi\left(\dfrac{\hat{\theta}-\theta}{s_{j}}\right)d\hat{\theta}dG(\theta)}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Risk-minimizing decision rule:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\delta^{*}={\displaystyle \arg\min_{\delta\in\mathcal{D}}}{\displaystyle \ \sum_{j}\int}{\displaystyle \int\mathcal{L}(\theta,\delta(\hat{\theta},s_{j}))\dfrac{1}{s_{j}}\phi\left(\dfrac{\hat{\theta}-\theta}{s_{j}}\right)d\hat{\theta}dG(\theta)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Optimal Decision Rule
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
With type I/type II loss function, solution is to select schools with sufficient
ly high posterior probability of value-added below 
\begin_inset Formula $c$
\end_inset

:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\delta^{*}(\hat{\theta}_{j},s_{j})=1\left\{ \Pr_{G}\left[\theta_{j}<c|\hat{\theta}_{j},s_{j}\right]\geq\dfrac{1}{1+\kappa}\right\} $
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
This means we should select based on posterior 
\begin_inset Formula $(1/(1+\kappa))$
\end_inset

 quantile rather than posterior mean
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
In normal/normal model:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\delta^{*}(\hat{\theta}_{j},s_{j})=1\left\{ \left(\tfrac{\sigma_{\theta}^{2}}{\sigma_{\theta}^{2}+s_{j}^{2}}\right)\hat{\theta}_{j}+\left(\tfrac{s_{j}^{2}}{\sigma_{\theta}^{2}+s_{j}^{2}}\right)\mu_{\theta}+\sqrt{\tfrac{\sigma_{\theta}^{2}s_{j}^{2}}{\sigma_{\theta}^{2}+s_{j}^{2}}}\Phi^{-1}\left(\dfrac{1}{1+\kappa}\right)\leq c\right\} $
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
EB decision rule 
\begin_inset Formula $\hat{\delta}^{*}(\hat{\theta}_{j},s_{j})$
\end_inset

 plugs in estimates 
\begin_inset Formula $(\hat{\mu}_{\theta},\hat{\sigma}_{\theta})$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EB Decision Rule: Discussion
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\hat{\delta}^{*}(\hat{\theta}_{j},s_{j})=1\left\{ \left(\tfrac{\hat{\sigma}_{\theta}^{2}}{\hat{\sigma}_{\theta}^{2}+s_{j}^{2}}\right)\hat{\theta}_{j}+\left(\tfrac{s_{j}^{2}}{\hat{\sigma}_{\theta}^{2}+s_{j}^{2}}\right)\hat{\mu}_{\theta}+\sqrt{\tfrac{\hat{\sigma}_{\theta}^{2}s_{j}^{2}}{\hat{\sigma}_{\theta}^{2}+s_{j}^{2}}}\Phi^{-1}\left(\dfrac{1}{1+\kappa}\right)\leq c\right\} $
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Adjust posterior mean by 
\begin_inset Formula $\sqrt{\sigma_{\theta}^{2}s_{j}^{2}/(\sigma_{\theta}^{2}+s_{j}^{2})}\Phi^{-1}\left(1/(1+\kappa)\right)$
\end_inset

, which may be positive or negative depending on costs of type I vs.
 II errors
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Punish schools with larger standard errors if we care more about failing
 to close low performers
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Reward schools with larger standard errors if we care more about mistakenly
 closing high performers
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
Note that schools with the same posterior mean will be treated differently
 based on standard errors – raises potential horizontal equity issues
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
General principle: different objectives call for using different features
 of posterior for decision-making
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
It's important to state your loss function!
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Posterior Mean Decision Frontier: 
\begin_inset Formula $c=\mu_{\theta}+\sigma_{\theta}\Phi^{-1}(0.25)$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename post_mean_rule.png
	lyxscale 50
	scale 39

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Posterior 90th Percentile Rule Rewards Large 
\begin_inset Formula $s_{j}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename post_p90_rule.png
	lyxscale 50
	scale 39

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Posterior 10th Percentile Rule Penalizes Large 
\begin_inset Formula $s_{j}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename post_p10_rule.png
	lyxscale 50
	scale 39

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Individualized Treatment Effect Predictions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Another increasingly common use case: individualized treatment effect prediction
s or treatment recommendations
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Consider a randomized controlled trial of a binary treatment 
\begin_inset Formula $T_{i}\in\{0,1\}$
\end_inset


\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Potential outcomes 
\begin_inset Formula $Y_{i}(1)$
\end_inset

 and 
\begin_inset Formula $Y_{i}(0)$
\end_inset

 describe 
\begin_inset Formula $i$
\end_inset

's outcomes in the treated and untreated states
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Suppose each individual also belongs to one of 
\begin_inset Formula $G$
\end_inset

 subgroups, indicated by 
\begin_inset Formula $G_{i}\in\{1,...,G\}$
\end_inset


\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Randomization 
\begin_inset Formula $\implies$
\end_inset

 treatment is independent of potential outcomes within each group: 
\begin_inset Formula $(Y_{i}(1),Y_{i}(0))\Perp T_{i}|G_{i}$
\end_inset


\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Treatment/control comparisons in each group therefore identify conditional
 average treatment effects (CATEs) for each group:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $E[Y_{i}|T_{i}=1,G_{i}=g]-E[Y_{i}|T_{i}=0,G_{i}=g]=E[Y_{i}(1)-Y_{i}(0)|G_{i}=g]\equiv\Delta_{g}$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Individualized Treatment Effect Predictions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Treatment/control contrasts yield estimates 
\begin_inset Formula $\hat{\Delta}_{g}$
\end_inset

 and standard errors 
\begin_inset Formula $s_{g}$
\end_inset


\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
If asked to provide a treatment effect prediction for each group, one option
 is to report 
\begin_inset Formula $\hat{\Delta}_{g}$
\end_inset

.
 Can we do better?
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Linear shrinkage estimate of CATE for group 
\begin_inset Formula $g$
\end_inset

:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\hat{\Delta}_{g}^{*}=\left(\tfrac{\hat{\sigma}_{\Delta}^{2}}{\hat{\sigma}_{\Delta}^{2}+s_{g}^{2}}\right)\hat{\Delta}_{g}+\left(\tfrac{s_{g}^{2}}{\hat{\sigma}_{\Delta}^{2}+s_{g}^{2}}\right)\hat{\mu}_{\Delta}$
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Shrinkage reduces MSE by 
\begin_inset Quotes eld
\end_inset

borrowing strength
\begin_inset Quotes erd
\end_inset

 from overall average treatment effect estimate 
\begin_inset Formula $\hat{\mu}_{\Delta}$
\end_inset

, avoiding excess weight on small/noisy subgroups
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
If we want to recommend the treatment with higher mean potential outcome
 for each group, may want to base recommendations on posterior probability
 of positive CATE
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
With normal prior 
\begin_inset Formula $G$
\end_inset

, 
\begin_inset Formula $Pr_{\hat{G}}[\Delta_{g}>0|\hat{\Delta}_{g},s_{g}]=\Phi\left(\tfrac{\hat{\Delta}_{g}^{*}}{[\hat{\sigma}_{\Delta}^{2}s_{g}^{2}/(\hat{\sigma}_{\Delta}^{2}+s_{g}^{2})]}\right)$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EB and Machine Learning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
EB methods are closely related to 
\series bold
machine learning
\series default
 (ML) approaches
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
ML refers to a suite of tools for model selection/penalization in settings
 with many predictors
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
LASSO, ridge regression, random forests, neural nets, transformers, etc.
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
Goal of ML is to reduce overfitting in finite samples, thereby obtaining
 a better estimate of some conditional distribution function
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
For example, estimate 
\begin_inset Formula $E[Y_{i}|X_{i}]$
\end_inset

 when number of regressors in 
\begin_inset Formula $X_{i}$
\end_inset

 is bigger than sample size
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
Sounds a lot like using EB shrinkage to reduce variance and improve MSE....
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EB and Machine Learning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
To draw connections between EB and ML, return to parametric normal/normal
 model with 
\begin_inset Formula $N$
\end_inset

 observations per group:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $Y_{ij}={\displaystyle \theta_{j}+\varepsilon_{ij}}$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\varepsilon_{ij}|\theta_{j}\sim N(0,\sigma_{\epsilon}^{2})$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\theta_{j}\sim N(0,\sigma_{\theta}^{2})$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Unbiased estimator 
\begin_inset Formula $\hat{\theta}_{j}=\tfrac{1}{N}\sum_{i}Y_{ij}$
\end_inset

 with sampling variance 
\begin_inset Formula $s_{j}^{2}=\sigma_{\epsilon}^{2}/N$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Posterior distribution for 
\begin_inset Formula $\theta_{j}$
\end_inset

 is 
\begin_inset Formula $N(\theta_{j}^{*},V^{*})$
\end_inset

 with
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\theta_{j}^{*}=\left(\tfrac{\sigma_{\theta}^{2}}{\sigma_{\theta}^{2}+\sigma_{\epsilon}^{2}/N}\right)\hat{\theta}_{j},\ V^{*}=\tfrac{\sigma_{\epsilon}^{2}\sigma_{\theta}^{2}}{N\sigma_{\theta}^{2}+\sigma_{\epsilon}^{2}}$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EB and Machine Learning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Posterior density for 
\begin_inset Formula $\theta_{j}$
\end_inset

:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $f(\theta_{j}|Y_{1j},....,Y_{Nj})=\dfrac{\left[{\displaystyle \prod_{i=1}^{N}\tfrac{1}{\sigma_{\epsilon}}\phi}\left(\tfrac{Y_{ij}-\theta_{j}}{\sigma_{\epsilon}}\right)\right]\tfrac{1}{\sigma_{\theta}}\phi\left(\tfrac{\theta_{j}}{\sigma_{\theta}}\right)}{\int_{-\infty}^{\infty}\left[{\displaystyle \prod_{i=1}^{N}\tfrac{1}{\sigma_{\epsilon}}\phi}\left(\tfrac{Y_{ij}-\theta}{\sigma_{\epsilon}}\right)\right]\tfrac{1}{\sigma_{\theta}}\phi\left(\tfrac{\theta}{\sigma_{\theta}}\right)d\theta}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Posterior distribution is normal 
\begin_inset Formula $\implies$
\end_inset

 posterior mean and mode coincide
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
This implies posterior means maximize log posterior density:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $(\theta_{1}^{*},...,\theta_{J}^{*})={\displaystyle \arg\max_{(\theta_{1},...,\theta_{J})}}\ {\displaystyle \sum_{j}}\log f(\theta_{j}|Y_{1j}....Y_{Nj})$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $={\displaystyle \arg\max_{(\theta_{1},...,\theta_{J})}}\ {\displaystyle \sum_{j=1}^{J}\sum_{i=1}^{N}\log}\phi\left(\tfrac{Y_{ij}-\theta_{j}}{\sigma_{\epsilon}}\right)+{\displaystyle \sum_{j=1}^{J}}\log\phi\left(\tfrac{\theta_{j}}{\sigma_{\theta}}\right)+cons$
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Posterior mode is also known as a 
\series bold
maximum a posteriori 
\series default
(MAP) estimate
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EB and Machine Learning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Plugging in normal density yields
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $(\theta_{1}^{*},...,\theta_{J}^{*})={\displaystyle \arg\max_{(\theta_{1},...,\theta_{J})}}\ {\displaystyle -\sum_{j=1}^{J}\sum_{i=1}^{N}\dfrac{(Y_{ij}-\theta_{j})^{2}}{2\sigma_{\epsilon}^{2}}}-{\displaystyle \sum_{j=1}^{J}}\dfrac{\theta_{j}^{2}}{2\sigma_{\theta}^{2}}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $={\displaystyle \arg\min_{(\theta_{1},...,\theta_{J})}}\ {\displaystyle \sum_{j=1}^{J}\sum_{i=1}^{N}(Y_{ij}-\theta_{j})^{2}}+\dfrac{\sigma_{\epsilon}^{2}}{\sigma_{\theta}^{2}}{\displaystyle \sum_{j=1}^{J}}\theta_{j}^{2}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $={\displaystyle \arg\min_{(\theta_{1},...,\theta_{J})}}\ {\displaystyle \sum_{j=1}^{J}\sum_{i=1}^{N}(Y_{ij}-\theta_{j})^{2}}+\lambda p(\theta_{1},...,\theta_{J})$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
This is regularized least squares with an L2 (quadratic) penalty 
\begin_inset Formula $p(\cdot)$
\end_inset

, also known as 
\series bold
ridge regression
\series default

\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Empirical Bayes 
\begin_inset Formula $\implies$
\end_inset

 use the data to choose tuning parameter 
\begin_inset Formula $\lambda$
\end_inset

 in penalty function (i.e.
 estimate 
\begin_inset Formula $\sigma_{\epsilon}^{2}/\sigma_{\theta}^{2}$
\end_inset

)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EB and Machine Learning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
ML penalization/regularization procedures often have an EB interpretation
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Ridge regression estimates (L2 penalization) can be interpreted as posterior
 means from a model with normal priors
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
LASSO estimates (L1 penalization) can be interpreted as MAP estimates from
 a model with double exponential (Laplace) priors 
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
When doing model selection or penalization via ML, useful to think about
 implicit prior distribution and connection to loss function
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
See Abadie and Kasy (2019) for analysis of the relative performance of common
 regularization approaches under various 
\begin_inset Formula $G$
\end_inset

's
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Empirical Bayes vs.
 Full Bayes
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
A fully Bayesian analysis would add a third level to the hierarchy: a 
\series bold
hyperprior
\series default
 over the mixing distribution, with parameters chosen by the researcher
 rather than estimated
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
For example, we might have a normal/normal model with a normal-inverse Gamma
 hyperprior:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\theta}_{j}|\theta_{j},s_{j}\sim N(\theta_{j},s_{j}^{2})$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\theta_{j}|s_{j}\sim N(\mu_{\theta},\sigma_{\theta}^{2})$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $(\mu_{\theta},\sigma_{\theta}^{2})|\mathbf{s}\sim N-\Gamma^{-1}(m,\lambda,\alpha,\beta)$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
We would then choose values for (
\begin_inset Formula $m,\lambda,\alpha,\beta)$
\end_inset

 and compute posterior distributions for 
\begin_inset Formula $\mu_{\theta}$
\end_inset

, 
\begin_inset Formula $\sigma_{\theta}$
\end_inset

, and each 
\begin_inset Formula $\theta_{j}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Why should we opt for empirical Bayes rather than fully Bayesian methods?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EB vs.
 Full B
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Pros of full Bayes:
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Internally consistent and coherent approach to updating beliefs about parameters
 (if you're a Bayesian).
 But if you're a frequentist....
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
EB posteriors do not account for estimation error in hyperparameters, so
 can overstate precision (though we can adjust for this)
\begin_inset VSpace medskip
\end_inset


\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
In some cases hyperparameters are difficult to estimate, and smoothing via
 a hyperprior can help
\begin_inset VSpace medskip
\end_inset


\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
Cons of full Bayes:
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Fully Bayesian estimation often requires simulation methods (Markov Chain
 Monte Carlo, MCMC), which are harder to implement and less transparent
\begin_inset VSpace medskip
\end_inset


\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Where do the parameters of the hyperprior come from? To the extent that
 these affect the estimates, why should we believe the results?
\begin_inset VSpace medskip
\end_inset


\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
EB estimates have desirable frequentist properties and are easier to understand
 – arguably less 
\begin_inset Quotes eld
\end_inset

harmful
\begin_inset Quotes erd
\end_inset


\begin_inset VSpace medskip
\end_inset


\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
If hyperparameters are estimated precisely, there won't be much difference
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Empirical Bayes Extensions
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Robust/Non-parametric Empirical Bayes
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Our discussion so far has focused on parametric models with normality/independen
ce assumptions
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Though we've seen EB procedures can be robust to violations of these assumptions
 (James/Stein; linear shrinkage as best linear approximation)
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
Next, consider robust/non-parametric EB approaches that relax these assumptions
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Generalized variance estimation
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Precision-dependence
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Non-parametric deconvolution
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Linear vs.
 non-parametric shrinkage
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
Main example: study of employer-level labor market discrimination by Kline,
 Rose and Walters (2022)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generalized Variance Estimation
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Suppose we are interested in a vector of 
\begin_inset Formula $J$
\end_inset

 parameters 
\begin_inset Formula $\Theta=(\theta_{1},...,\theta_{J})^{\prime}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
We have estimates 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 collected in vector 
\begin_inset Formula $\hat{\Theta}=(\hat{\theta}_{1},...,\hat{\theta}_{J})^{\prime}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Assume 
\begin_inset Formula $\hat{\Theta}$
\end_inset

 is an unbiased estimate of 
\begin_inset Formula $\Theta$
\end_inset

: 
\begin_inset Formula $E[\hat{\Theta}|\Theta]=\Theta$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Matrix 
\begin_inset Formula $V$
\end_inset

 describes the variance of noise in 
\begin_inset Formula $\hat{\Theta}$
\end_inset

:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $V=E\left[(\hat{\Theta}-\Theta)(\hat{\Theta}-\Theta)^{\prime}|\Theta\right]$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
With independence across units, 
\begin_inset Formula $V$
\end_inset

 has 
\begin_inset Formula $s_{j}^{2}$
\end_inset

's on the diagonal and zeros elsewhere.
 In other cases, off-diagonal elements may not be zero
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Correlation Across Units
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
When should we worry about correlation in noise across units? 
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Suppose we fit OLS VAMs for two outcomes, 
\begin_inset Formula $Y_{i}^{1}$
\end_inset

 and 
\begin_inset Formula $Y_{i}^{2}$
\end_inset

 (e.g.
 test scores and social skills):
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $Y_{i}^{k}=\sum_{j}\theta_{j}^{k}D_{ij}+X_{i}^{\prime}\beta^{k}+\epsilon_{i}^{k},\ k\in\{1,2\}$
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Here the full vector of estimates is 
\begin_inset Formula $\hat{\Theta}=(\hat{\theta}_{1}^{1},...,\hat{\theta}_{J}^{1},\hat{\theta}_{1}^{2},...,\hat{\theta}_{J}^{2})^{\prime}$
\end_inset


\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
\begin_inset Formula $\hat{\theta}_{j}^{1}$
\end_inset

 and 
\begin_inset Formula $\hat{\theta}_{j}^{2}$
\end_inset

 are estimated from the same data so likely highly correlated – 
\begin_inset Formula $V$
\end_inset

 is not diagonal
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
We will need to account for this correlation if we want to study relationships
 between parameters across equations (e.g.
 covariance between 
\begin_inset Formula $\theta_{j}^{1}$
\end_inset

 and 
\begin_inset Formula $\theta_{j}^{2}$
\end_inset

)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Quadratic Forms
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Suppose we are interested in a quadratic form involving 
\begin_inset Formula $\Theta$
\end_inset

:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\delta=\Theta^{\prime}A\Theta$
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
\begin_inset Formula $A$
\end_inset

 is some known 
\begin_inset Formula $J\times J$
\end_inset

 matrix
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
For example, we might have 
\begin_inset Formula $A=J^{-1}\left(I_{J}-J^{-1}\iota_{J}\iota_{J}^{\prime}\right)$
\end_inset

, with 
\begin_inset Formula $I_{J}$
\end_inset

 the 
\begin_inset Formula $J\times J$
\end_inset

 identity and 
\begin_inset Formula $\iota_{J}$
\end_inset

 a 
\begin_inset Formula $J\times1$
\end_inset

 vector of 1's
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
With this choice of 
\begin_inset Formula $A$
\end_inset

, 
\begin_inset Formula $\delta$
\end_inset

 is the variance of 
\begin_inset Formula $\theta_{j}$
\end_inset

's across units:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\delta=\dfrac{1}{J}{\displaystyle \sum_{j=1}^{J}(\theta_{j}-\bar{\theta})^{2},\ \bar{\theta}=\dfrac{1}{J}\sum_{j=1}^{J}}\theta_{j}$
\end_inset

 
\end_layout

\begin_layout Itemize

\size scriptsize
In the two-equation example, we can obtain 
\begin_inset Formula $Cov(\theta_{j}^{1},\theta_{j}^{2})$
\end_inset

 by choosing 
\begin_inset Formula $A=\left[\begin{array}{cc}
0 & J^{-1}\left(I_{J}-J^{-1}\iota_{J}\iota_{J}^{\prime}\right)\\
0 & 0
\end{array}\right]$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Plug-in Estimator
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Plug-in estimator of quadratic form 
\begin_inset Formula $\delta$
\end_inset

:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\delta}=\hat{\Theta}^{\prime}A\hat{\Theta}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
This estimator is biased: 
\begin_inset Formula $E[\hat{\delta}|\Theta]\neq\delta$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Intuitively, 
\begin_inset Formula $\hat{\theta}_{j}^{2}$
\end_inset

 is an upward-biased estimate of 
\begin_inset Formula $\theta_{j}^{2}$
\end_inset

 due to noise
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Even if all 
\begin_inset Formula $\theta_{j}$
\end_inset

's were equal, we'd get some variation in the 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

's by chance
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Plug-in estimator does not account for the contribution of sampling error
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Generalization of the idea that variance of 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

's is too big relative to true 
\begin_inset Formula $\theta_{j}$
\end_inset

's
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bias of Plug-in Estimator
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Formally, expectation of the plug-in estimator is
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $E[\hat{\delta}|\Theta]=E[\hat{\Theta}^{\prime}A\hat{\Theta}|\Theta]$
\end_inset

 
\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $=E\left[\left(\Theta+(\hat{\Theta}-\Theta)\right)^{\prime}A\left(\Theta+(\hat{\Theta}-\Theta)\right)|\Theta\right]$
\end_inset


\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $=\Theta^{\prime}A\Theta+\Theta^{\prime}AE\left[\hat{\Theta}-\Theta|\Theta\right]+E\left[\hat{\Theta}-\Theta|\Theta\right]^{\prime}A\Theta+E\left[(\hat{\Theta}-\Theta)^{\prime}A(\hat{\Theta}-\Theta)|\Theta\right]$
\end_inset


\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $=\delta+E\left[tr\left((\hat{\Theta}-\Theta)^{\prime}A(\hat{\Theta}-\Theta)\right)|\Theta\right]$
\end_inset


\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $=\delta+tr\left(AE\left[(\hat{\Theta}-\Theta)(\hat{\Theta}-\Theta)^{\prime}|\Theta\right]\right)$
\end_inset


\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $=\delta+tr(AV)$
\end_inset

.
\size footnotesize

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Bias of plug-in estimator is therefore 
\begin_inset Formula $E[\hat{\delta}-\delta|\Theta]=tr\left(AV\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bias-Corrected Variance Estimation
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
A bias-corrected estimator of 
\begin_inset Formula $\delta$
\end_inset

 using a variance estimate 
\begin_inset Formula $\hat{V}$
\end_inset

 is given by
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\delta}_{BC}=\hat{\delta}-tr(A\hat{V})=\hat{\Theta}^{\prime}A\hat{\Theta}-tr(A\hat{V})$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Subtract off expected contribution of noise from naive plug-in estimator
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Analogous to subtracting off average squared SE from sample variance of
 estimates in earlier example
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
If 
\begin_inset Formula $\hat{V}$
\end_inset

 is an unbiased estimate of the variance 
\begin_inset Formula $V$
\end_inset

 so that 
\begin_inset Formula $E[\hat{V}]=V$
\end_inset

, then 
\begin_inset Formula $\hat{\delta}_{BC}$
\end_inset

 is unbiased for 
\begin_inset Formula $\delta$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Kline, Saggio and Sølvsten (KSS, 2020) propose leave-out approach to obtain
 finite-sample unbiased estimate of 
\begin_inset Formula $V$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Precision-Weighting
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
So far we have assumed that effect sizes 
\begin_inset Formula $\theta_{j}$
\end_inset

 are independent of sampling variance 
\begin_inset Formula $s_{j}^{2}$
\end_inset

 across units
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
If 
\begin_inset Formula $\theta_{j}$
\end_inset

 and 
\begin_inset Formula $s_{j}^{2}$
\end_inset

 are independent, we may want to weight estimates of mean and/or variance
 of 
\begin_inset Formula $G$
\end_inset

 to account for differences in noise:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\hat{\mu}_{\theta}={\displaystyle \sum_{j=1}^{J}w_{j}\hat{\theta}_{j},\ \sum_{j=1}^{J}w_{j}=1}$
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Precision-weighting uses weight proportional to 
\begin_inset Formula $1/s_{j}^{2}$
\end_inset


\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Optimal (minimum-variance) weight is proportional to 
\begin_inset Formula $1/(\sigma_{\theta}^{2}+s_{j}^{2})$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Can form estimates of these weights based on first-step unweighted 
\begin_inset Formula $\hat{\sigma}_{\theta}^{2}$
\end_inset

 (analogous to FGLS)
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Maximum likelihood jointly estimates 
\begin_inset Formula $(\mu_{\theta},\sigma_{\theta}^{2})$
\end_inset

 and optimal weights in one step (analogous to CUE)
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
But precision-weighting changes the estimand if 
\begin_inset Formula $\theta_{j}$
\end_inset

 is correlated with 
\begin_inset Formula $s_{j}$
\end_inset

, which some find unappealing
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
An instance of general debate about weighting in econometrics
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Interpreting Precision-Dependence
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Why would effect sizes 
\begin_inset Formula $\theta_{j}$
\end_inset

 be related to sampling variances 
\begin_inset Formula $s_{j}^{2}$
\end_inset

?
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Intuition: Recall that for sample mean, 
\begin_inset Formula $s_{j}^{2}=\sigma_{j}^{2}/N_{j}$
\end_inset


\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Units with larger sample sizes 
\begin_inset Formula $N_{j}$
\end_inset

 may have bigger/smaller effects
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Units with more within-group variance 
\begin_inset Formula $\sigma_{j}^{2}$
\end_inset

 may have bigger/smaller effect
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
In some applications, precision-dependence can be economically interesting
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
For example, teacher value-added literature finds that value-added increases
 with experience 
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
This suggests that more experienced teachers will have higher 
\begin_inset Formula $\theta_{j}$
\end_inset

 and more data available to estimate it (lower 
\begin_inset Formula $s_{j}^{2}$
\end_inset

)
\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
Later we will return to approaches to dealing with precision-dependence
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Application: Employer-level Labor Market Discrimination
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Example of robust/non-parametric EB techniques comes from a study of employment
 discrimination by Kline, Rose, and Walters (2022)
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Massive resume correspondence study sending applications to multiple establishme
nts at large employers
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
108 Fortune 500 firms
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Up to 125 jobs per firm, each in a different county
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
8 applications per job (stratified 4 Black/4 white)
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
Following Bertrand and Mullainathan (2004), manipulate employer perceptions
 of race and sex using distinctive names
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setbeamercolor{background canvas}{bg=} 
\backslash
includepdf[pages=8]{randres_slides_final.pdf}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setbeamercolor{background canvas}{bg=} 
\backslash
includepdf[pages=10]{randres_slides_final.pdf}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setbeamercolor{background canvas}{bg=} 
\backslash
includepdf[pages=12]{randres_slides_final.pdf}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setbeamercolor{background canvas}{bg=} 
\backslash
includepdf[pages=15]{randres_slides_final.pdf}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Average Contact Gaps by Race and Gender
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename mean_diffs.png
	lyxscale 50
	scale 40

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Job-level Estimates
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Let 
\begin_inset Formula $Y_{ijf}(r)\in\{0,1\}$
\end_inset

 indicate potential callback to applicant 
\begin_inset Formula $i$
\end_inset

 at job 
\begin_inset Formula $j$
\end_inset

 within firm 
\begin_inset Formula $f$
\end_inset

 if assigned race 
\begin_inset Formula $r\in\{b,w\}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Average treatment effect at this job is 
\begin_inset Formula $\Delta_{jf}\equiv E[Y_{ijf}(w)-Y_{ijf}(b)]$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Observed outcome is 
\begin_inset Formula $Y_{ijf}=Y_{ijf}(R_{ijf})$
\end_inset

, with 
\begin_inset Formula $R_{ijf}\in\{b,w\}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
White/Black difference in callback rates (
\series bold
contact gap
\series default
):
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\Delta}_{jf}=\dfrac{1}{4}\sum_{i=1}^{8}1\{R_{ijf}=w\}Y_{ijf}-\dfrac{1}{4}\sum_{i=1}^{8}1\{R_{ijf}=b\}Y_{ijf}$
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Random assignment of 
\begin_inset Formula $R_{ijf}$
\end_inset

 
\begin_inset Formula $\implies$
\end_inset

 
\begin_inset Formula $\hat{\Delta}_{jf}$
\end_inset

 is an unbiased estimate of 
\begin_inset Formula $\Delta_{jf}$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Firm-level Estimates
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Let 
\begin_inset Formula $\theta_{f}=E_{f}[\Delta_{jf}]$
\end_inset

 denote the average of 
\begin_inset Formula $\Delta_{jf}$
\end_inset

 across all jobs within firm 
\begin_inset Formula $f$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Observed average contact gap across the 
\begin_inset Formula $n_{f}$
\end_inset

 jobs at firm 
\begin_inset Formula $f$
\end_inset

:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\theta}_{f}=\dfrac{1}{n_{f}}{\displaystyle \sum_{j=1}^{n_{f}}\hat{\Delta}_{jf}}$
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Random sampling of jobs 
\begin_inset Formula $\implies$
\end_inset

 
\begin_inset Formula $\hat{\theta}_{f}$
\end_inset

 is an unbiased estimate of 
\begin_inset Formula $\theta_{f}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Unbiased (squared) standard error:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $s_{f}^{2}=\dfrac{1}{n_{f}(n_{f}-1)}{\displaystyle \sum_{j=1}^{J_{f}}(\hat{\Delta}_{jf}-\hat{\theta}_{f})^{2}}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
EB step 1: 
\begin_inset Formula $\{\hat{\theta}_{f},s_{f}\}_{f=1}^{F}$
\end_inset

 provide building blocks for analysis of firm heterogeneity
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Variance of Discrimination
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Let 
\begin_inset Formula $G$
\end_inset

 denote the distribution of contact gaps across firms: 
\begin_inset Formula $\theta_{f}\sim G$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Estimators for mean and variance of 
\begin_inset Formula $G$
\end_inset

:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\hat{\mu}_{\theta}=\dfrac{1}{F}{\displaystyle \sum_{f=1}^{F}\hat{\theta}_{f},}$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\hat{\sigma}_{\theta}^{2}=\dfrac{1}{F}{\displaystyle \sum_{f=1}^{F}\left[\left(\hat{\theta}_{f}-\hat{\mu}_{\theta}\right)^{2}-\left(\dfrac{F-1}{F}\right)s_{f}^{2}\right]}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
\begin_inset Formula $\hat{\sigma}_{\theta}^{2}$
\end_inset

 is a special case of Kline, Saggio and Sølvsten (2020) unbiased variance
 estimator
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Unbiased 
\begin_inset Formula $s_{f}^{2}$
\end_inset

+ degrees of freedom correction 
\begin_inset Formula $\implies$
\end_inset

 finite-sample unbiased estimate
\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
We can then form linear shrinkage estimates 
\begin_inset Formula $\hat{\theta}_{f}^{*}=\left(\tfrac{\hat{\sigma}_{\theta}^{2}}{\hat{\sigma}_{\theta}^{2}+s_{f}^{2}}\right)\hat{\theta}_{f}+\left(\tfrac{s_{f}^{2}}{\hat{\sigma}_{\theta}^{2}+s_{f}^{2}}\right)\hat{\mu}_{\theta}$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Standard Deviations of 
\begin_inset Formula $G$
\end_inset

: Substantial Variation for Both Race and Gender
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename means_table.pdf
	lyxscale 50
	scale 85

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Histogram of Unbiased Contact Gap Estimates
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename gap_hist.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Unbiased vs.
 Linear Shrinkage Contact Gap Estimates
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename shrunk_hist.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Non-Parametric Deconvolution
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
To get a richer picture of 
\begin_inset Formula $G$
\end_inset

, return to hierarchical random effects framework with normal noise:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\hat{\theta}_{j}|\theta_{j},s_{j}\sim N(\theta_{j},s_{j}^{2})$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\theta_{j}\sim G$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
As before, think of normality of 
\begin_inset Formula $\hat{\theta}_{j}|\theta_{j},s_{j}$
\end_inset

 as an asymptotic approximation
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\series bold
\size scriptsize
Non-parametric deconvolution
\series default
: Estimate mixing distribution 
\begin_inset Formula $G$
\end_inset

 under minimal assumptions on its shape/properties
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
N.B.: Need to account for any dependence between effect sizes 
\begin_inset Formula $\theta_{j}$
\end_inset

 and sampling variances 
\begin_inset Formula $s_{j}^{2}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Two approaches to non-parametric deconvolution:
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Non-parametric maximum likelihood
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Log-spline deconvolution
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Non-Parametric Maximum Likelihood
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Classic deconvolution approach:
\series bold
 Non-parametric maximum likelihood
\series default
 
\series bold
estimator
\series default
 (NPMLE; Robbins, 1950; Kiefer and Wolfowitz, 1956; Heckman and Singer,
 1984)
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
NPMLE picks mixing distribution to maximize likelihood of observed data:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula ${\displaystyle \hat{G}}={\displaystyle \arg\max_{G\in\mathcal{G}}}{\displaystyle \sum_{j=1}^{J}}\log\left({\displaystyle \int\dfrac{1}{s_{j}}\phi\left(\dfrac{\hat{\theta}_{j}-\theta}{s_{j}}\right)dG(\theta)}\right)$
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Solution is a discrete distribution 
\begin_inset Formula $\hat{G}$
\end_inset

 with at most 
\begin_inset Formula $J$
\end_inset

 mass points
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Koenker and Mizera (2014) develop an approximation that is straightforward
 to compute with modern convex optimization methods
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Implemented in 
\series bold
REBayes
\series default
 R package (Koenker and Gu, 2017)
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Log-Spline Deconvolution
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Efron (2016) proposes to approximate 
\begin_inset Formula $G$
\end_inset

 with distribution in smooth exponential family with log density parameterized
 by a natural spline
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
For a set of 
\begin_inset Formula $M$
\end_inset

 support points 
\begin_inset Formula $(\bar{\theta}_{1},...,\bar{\theta}_{M})$
\end_inset

, suppose mass at point 
\begin_inset Formula $m$
\end_inset

 is given by
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $g_{m}(\alpha)=\exp\left(q_{m}^{\prime}\alpha-\log\left(\sum_{\ell=1}^{M}\exp(q_{\ell}^{\prime}\alpha)\right)\right)$
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
\begin_inset Formula $q_{m}$
\end_inset

 is a 
\begin_inset Formula $B\times1$
\end_inset

 vector of values of natural spline basis functions; 
\begin_inset Formula $\alpha$
\end_inset

 is a 
\begin_inset Formula $B\times1$
\end_inset

 parameter vector
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Estimate 
\begin_inset Formula $\alpha$
\end_inset

 by penalized maximum likelihood:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\hat{\alpha}={\displaystyle \arg\max_{\alpha}\ \sum_{j=1}^{J}\log\left(\sum_{m=1}^{M}g_{m}(\alpha)\dfrac{1}{s_{j}}\phi\left(\dfrac{\hat{\theta}_{j}-\bar{\theta}_{m}}{s_{j}}\right)\right)}-c\sqrt{\alpha^{\prime}\alpha}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Requires choosing tuning parameters: penalty 
\begin_inset Formula $c$
\end_inset

, number and range of support points 
\begin_inset Formula $\bar{\theta}_{m}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Homoskedastic-noise version implemented in 
\series bold
deconvolveR
\series default
 R package (Narasimhan and Efron, 2020)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Histogram of Race Contact Gap Estimates
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename hist_gapswhite.pdf
	lyxscale 50
	scale 39

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Log-spline Deconvolution Estimate for Race
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename race_density_norestriction.pdf
	lyxscale 50
	scale 85

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Deconvolution Imposing Shape Restriction: 
\begin_inset Formula $\theta_{f}\geq0$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename race_density.pdf
	lyxscale 50
	scale 135

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
NPMLE Deconvolution Estimate for Race
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename race_npmle.pdf
	lyxscale 50
	scale 78

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Histogram of Gender Contact Gap Estimates
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename hist_gapsmale.pdf
	lyxscale 50
	scale 39

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Log-spline Deconvolution Estimate for Gender
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename gender_density.pdf
	lyxscale 50
	scale 135

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
NPMLE Deconvolution Estimate for Gender
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename gender_npmle.pdf
	lyxscale 50
	scale 80

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Lorenz Curves Derived from Log-spline 
\begin_inset Formula $\hat{G}$
\end_inset

's
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename lorenz.pdf
	lyxscale 50
	scale 95

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Non-Parametric Shrinkage
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
After obtaining 
\begin_inset Formula $\hat{G}$
\end_inset

 via non-parametric deconvolution, we can form non-parametric posteriors
 using posterior density:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\hat{f}\left(\theta_{j}|\hat{\theta}_{j},s_{j}\right)=\dfrac{\tfrac{1}{s_{j}}\phi\left(\tfrac{\hat{\theta}_{j}-\theta_{j}}{s_{j}}\right)d\hat{G}\left(\theta_{j}\right)}{\int\tfrac{1}{s_{j}}\phi\left(\tfrac{\hat{\theta}_{j}-\theta}{s_{j}}\right)d\hat{G}(\theta)}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Non-parametric posterior mean 
\begin_inset Formula $\hat{\theta}_{j}^{*}=\int\theta\hat{f}(\theta|\hat{\theta}_{j},s_{j})d\theta$
\end_inset

 will generally differ from linear shrinkage estimate 
\begin_inset Formula $\tilde{\theta}_{j}=\left(\tfrac{\hat{\sigma}_{\theta}^{2}}{\hat{\sigma}_{\theta}^{2}+s_{j}^{2}}\right)\hat{\theta}_{j}+\left(\tfrac{s_{j}^{2}}{\hat{\sigma}_{\theta}^{2}+s_{j}^{2}}\right)\hat{\mu}_{\theta}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
\begin_inset Formula $\hat{\theta}_{j}^{*}$
\end_inset

 may be more accurate (lower MSE) if 
\begin_inset Formula $G$
\end_inset

 is not normal
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
On the other hand, linear shrinkage only requires estimating mean and variance
 of 
\begin_inset Formula $G$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
\begin_inset Formula $\tilde{\theta}_{j}$
\end_inset

is best linear approximation to true unknown posterior mean 
\begin_inset Formula $\theta_{j}^{*}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
May perform better than non-linear estimate 
\begin_inset Formula $\hat{\theta}_{j}^{*}$
\end_inset

 if higher moments of 
\begin_inset Formula $G$
\end_inset

 are poorly-estimated
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Histogram of Posterior Means for Race
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename race_post_means.pdf
	lyxscale 50
	scale 135

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Histogram of Posterior Means for Gender
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename gender_hist.pdf
	lyxscale 50
	scale 135

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Deconvolution with Precision-Dependence
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
With precision-dependence, we need to estimate conditional mixing distribution
 
\begin_inset Formula $G(\theta|s_{j})$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
One approach is to assume or estimate a model of the relationship between
 
\begin_inset Formula $\theta_{j}$
\end_inset

 and 
\begin_inset Formula $s_{j}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
For example, we might allow dependence only through the mean:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $E[\theta_{j}|s_{j}]=\alpha+\beta\log s_{j}$
\end_inset

,
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $r_{j}\equiv\theta_{j}-\alpha-\beta\log s_{j}$
\end_inset

,
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $r_{j}|s_{j}\sim G_{r}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Regress 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 on 
\begin_inset Formula $\log s_{j}$
\end_inset

, then deconvolve residual 
\begin_inset Formula $\hat{r}_{j}$
\end_inset

 to estimate 
\begin_inset Formula $G_{r}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Can generalize to location/scale models or more complicated models of dependence
 (Chen, 2023)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Using 
\begin_inset Formula $z$
\end_inset

-score Transformations
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Kline, Rose and Walters (2022) initially sidestep precision-dependence by
 transforming estimates to 
\begin_inset Formula $z$
\end_inset

-scores 
\begin_inset Formula $z_{f}=\hat{\theta}_{f}/s_{f}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
\begin_inset Formula $z_{f}\sim N(\mu_{f},1)$
\end_inset

, where 
\begin_inset Formula $\mu_{f}=\theta_{f}/s_{f}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Deconvolve 
\begin_inset Formula $z$
\end_inset

-scores to estimate distribution of 
\begin_inset Formula $\mu_{f}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
Then reconstruct distribution of 
\begin_inset Formula $\theta_{f}=s_{f}\mu_{f}$
\end_inset

 by a change of variables
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
If 
\begin_inset Formula $\mu_{f}$
\end_inset

 is independent of 
\begin_inset Formula $s_{f}$
\end_inset

, then 
\begin_inset Formula $g_{\theta}(\theta)=\int\dfrac{1}{s}g_{\mu}(\theta/s)h_{s}(s)ds$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
Note that independence of 
\begin_inset Formula $\mu_{f}$
\end_inset

 from 
\begin_inset Formula $s_{f}$
\end_inset

 implies 
\begin_inset Formula $E[\theta_{f}|s_{f}]=E[\mu_{f}]\times s_{f}$
\end_inset

, which is increasing in 
\begin_inset Formula $s_{f}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Seems to provide good empirical fit
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Alternative approach: deconvolve separately in bins of 
\begin_inset Formula $s_{f}$
\end_inset

; yields similar results
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Separate Deconvolutions for Low vs.
 High 
\begin_inset Formula $s_{f}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename conditional_densities.pdf
	lyxscale 50
	scale 80

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Marginal Distribution from Separate Deconvolutions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename separate_marginal.pdf
	lyxscale 50
	scale 70

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Variance-stabilizing Transformations
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
An alternative approach is to eliminate heteroskedasticity with a 
\series bold
variance-stabilizing transformation
\series default
 (VST)
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Suppose 
\begin_inset Formula $\hat{\theta}_{j}|\theta_{j},s_{j}^{2}\sim N(\theta_{j},s_{j}^{2})$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
In addition, suppose sampling variance 
\begin_inset Formula $s_{j}^{2}$
\end_inset

 is a known deterministic function of effect size 
\begin_inset Formula $\theta_{j}$
\end_inset

:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $s_{j}^{2}=h(\theta_{j})$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Define 
\begin_inset Formula $t(\theta)\equiv v\int_{-\infty}^{u}h(u)^{-1/2}du$
\end_inset

.
 By the delta method, we have
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $t(\hat{\theta}_{j})|\theta_{j}\sim N(t(\theta_{j}),v^{2})$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
We can then deconvolve the transformed variable 
\begin_inset Formula $t(\hat{\theta}_{j})$
\end_inset

, which has homoskedastic noise
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Binomial VST
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Suppose we have binomial data with success probability 
\begin_inset Formula $\theta_{j}$
\end_inset

 in group 
\begin_inset Formula $j$
\end_inset

 and 
\begin_inset Formula $N$
\end_inset

 trials per group:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $Y_{j}\sim Bin(N,\theta_{j})$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
The sample proportion of successes 
\begin_inset Formula $\hat{\theta}_{j}=Y_{j}/N$
\end_inset

 has 
\begin_inset Formula $E[\hat{\theta}_{j}|\theta_{j}]=\theta_{j}$
\end_inset

 and 
\begin_inset Formula $Var(\hat{\theta}_{j}|\theta_{j})=\theta_{j}(1-\theta_{j})/N$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Classic VST for a binomial (Bartlett, 1936, 1947; Anscomb, 1948):
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $t(\theta_{j})=\arcsin\sqrt{\theta_{j}}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
For the binomial, 
\begin_inset Formula $Var(t(\hat{\theta}_{j})|\theta_{j})=\dfrac{1}{4N}+O(N^{-2})$
\end_inset

, which no longer depends on 
\begin_inset Formula $\theta_{j}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Brown (2008) considers a wider class of VSTs for binomial models
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
VST Drawbacks
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
With more complicated data structures a known VST will typically not be
 available
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Can try to estimate a VST by fitting a model of the form 
\begin_inset Formula $s_{j}^{2}=h(\theta_{j};\gamma)$
\end_inset

 for unknown parameters 
\begin_inset Formula $\gamma$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
But then we are essentially back to modeling the dependence 
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
Outside of special cases it is also not clear when we should expect a determinis
tic relationship between precision and effect sizes
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Noise in Estimates of Precision
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
A related problem is that estimated standard errors 
\begin_inset Formula $s_{j}$
\end_inset

 may be noisy estimates of sampling uncertainty in 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

's
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Consider classic normal means problem with 
\begin_inset Formula $Y_{ij}\sim N(\theta_{j},\sigma_{j}^{2})$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Estimators of the mean and variance: 
\begin_inset Formula $\hat{\theta}_{j}=N_{j}^{-1}\sum_{i}Y_{ij}$
\end_inset

, 
\begin_inset Formula $\hat{\sigma}_{j}^{2}=(N_{j}-1)^{-1}\sum_{i}(Y_{ij}-\hat{\theta}_{j})^{2}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Distribution of estimated mean and variance:
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\theta}_{j}\sim N\left(\theta_{j},\sigma_{j}^{2}/N_{j}\right)$
\end_inset

, 
\begin_inset Formula $(N_{j}-1)\hat{\sigma}_{j}^{2}/\sigma_{j}^{2}\sim\chi_{N_{j}-1}^{2}$
\end_inset

,
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $\hat{\theta}_{j}\Perp\hat{\sigma}_{j}^{2}$
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
This implies squared SE 
\begin_inset Formula $\hat{s}_{j}^{2}=\hat{\sigma}_{j}^{2}/N_{j}$
\end_inset

 follows a Gamma distribution with shape 
\begin_inset Formula $(N_{j}-1)/2$
\end_inset

 and scale 
\begin_inset Formula $2\sigma_{j}^{2}/[N_{j}(N_{j}-1)]$
\end_inset

, independent of 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Dealing with Noisy Standard Errors
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
In more general settings we may have dependence between noise in 
\begin_inset Formula $\hat{\theta}_{j}$
\end_inset

 and 
\begin_inset Formula $\hat{s}_{j}$
\end_inset

, in addition to dependence between 
\begin_inset Formula $\theta_{j}$
\end_inset

 and 
\begin_inset Formula $s_{j}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Bivariate deconvolution: jointly deconvolve (
\begin_inset Formula $\hat{\theta}_{j},\hat{s}_{j})$
\end_inset

 to recover bivariate mixing distribution 
\begin_inset Formula $G(\theta,s)$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
NPMLE version implemented in 
\series bold
GLVmix
\series default
 R package of Koenker and Gu (2017) 
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
Could try multivariate VST (covariance-stabilizing transform), though this
 does not always exist (Holland, 1973)
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
In the normal means case, 
\begin_inset Formula $Var(\hat{s}_{j}^{2})=2\sigma_{j}^{4}/[N_{j}^{2}(N_{j}-1)]=2s_{j}^{4}/(N_{j}-1)$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Goes to zero with 
\begin_inset Formula $N_{j}$
\end_inset

 faster than 
\begin_inset Formula $s_{j}^{2}=\sigma_{j}^{2}/N_{j}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Perhaps for this reason, precision is typically treated as known in practice
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Section
Large-Scale Inference
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Large-Scale Inference
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Empirical Bayes methods are closely related to multiple testing approaches
 (
\begin_inset Quotes eld
\end_inset

large-scale inference;
\begin_inset Quotes erd
\end_inset

 Efron, 2012)
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Suppose we are interested in a null hypothesis involving 
\begin_inset Formula $\theta_{j}$
\end_inset

 for each unit, e.g., 
\begin_inset Formula $H_{0}:\ \theta_{j}=0$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Which subgroups are affected by an intervention?
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Which firms discriminate against Black applicants?
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
Let 
\begin_inset Formula $T_{j}\in\{0,1\}$
\end_inset

 denote an indicator equal to1 if the null is true for unit 
\begin_inset Formula $j$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Let 
\begin_inset Formula $R_{j}\in\{0,1\}$
\end_inset

 denote an indicator equal to 1 if we reject the null for unit 
\begin_inset Formula $j$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Multiple Testing
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Traditional hypothesis testing controls the probability of type I error
 (size) for a single test:
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Limit probability of a false rejection assuming the null is true
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
In other words, adopt a rejection rule such that 
\begin_inset Formula $\Pr[R_{j}=1|T_{j}=1]\leq\alpha$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
When we are interested in many hypotheses simultaneously, it is no longer
 clear what notion of error we should control
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Family-wise error rate (FWER): Probability of at least one mistaken rejection
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
False Discovery Rate (FDR): Expected share of rejections that are mistaken
\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
For large-scale testing problems FWER control can be very stringent
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
We will focus on controlling FDR, which is natural in the EB framework
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The False Discovery Rate
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Suppose our tests yield 
\begin_inset Formula $p$
\end_inset

-values 
\begin_inset Formula $p_{1},....,p_{J}$
\end_inset

, and we reject those with 
\begin_inset Formula $p_{j}\leq\bar{p}$
\end_inset

.
 How many mistakes do we expect to make?
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
By Bayes' rule, the share of true nulls among hypotheses we've rejected
 is:
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\Pr[T_{j}=1|p_{j}\leq\bar{p}]=\dfrac{\Pr[p_{j}\leq\bar{p}|T_{j}=1]\Pr[T_{j}=1]}{\Pr[p_{j}\leq\bar{p}]}$
\end_inset


\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $=\dfrac{\bar{p}\pi_{0}}{F_{p}(\bar{p})}$
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
This quantity is the 
\series bold
False Discovery Rate
\series default
 (FDR) for our decision rule (Benjamini and Hochberg, 1995)
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
FDR is the expected share of true nulls among the hypotheses we reject
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
If we can limit FDR, we can be reasonably confident that rejected hypotheses
 are false
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
FDR and 
\begin_inset Formula $G$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $FDR(\bar{p})=\dfrac{\bar{p}\pi_{0}}{F_{p}(\bar{p})}$
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
\begin_inset Formula $P$
\end_inset

-values are uniformly distributed under the null, so 
\begin_inset Formula $\Pr[p_{j}\leq\bar{p}|T_{j}=1]=\bar{p}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
The denominator is the marginal CDF of 
\begin_inset Formula $p$
\end_inset

-values, estimable from empirical share below 
\begin_inset Formula $\bar{p}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Key unknown quantity is 
\begin_inset Formula $\pi_{0}=\Pr[T_{j}=1]$
\end_inset

, the share of true nulls in the population
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
If 
\begin_inset Formula $\pi_{0}=1$
\end_inset

, all rejected hypotheses are true
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
If 
\begin_inset Formula $\pi_{0}=0$
\end_inset

, all rejected hypotheses are false, but so are all hypotheses we don't
 reject
\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size scriptsize
\begin_inset Formula $\pi_{0}$
\end_inset

 is a feature of 
\begin_inset Formula $G$
\end_inset

: 
\begin_inset Formula $\pi_{0}=\int1[\theta=0]dG(\theta)$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Can we use EB methods to get traction on 
\begin_inset Formula $\pi_{0}$
\end_inset

?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bounding 
\begin_inset Formula $\pi_{0}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $FDR(\bar{p})=\dfrac{\bar{p}\pi_{0}}{F_{p}(\bar{p})}$
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Conservative approach: plug in 
\begin_inset Formula $\pi_{0}=1$
\end_inset

 (Benjamini and Hochberg, 1995)
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Still implies low 
\begin_inset Formula $FDR$
\end_inset

 if many 
\begin_inset Formula $p$
\end_inset

-values close to 0 
\begin_inset Formula $(F_{p}(\bar{p})>>\bar{p})$
\end_inset


\begin_inset VSpace medskip
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
But we can do better
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Logically inconsistent to have 
\begin_inset Formula $\pi_{0}=1$
\end_inset

 but 
\begin_inset Formula $F_{p}(\bar{p})>>\bar{p}$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
\begin_inset Formula $\pi_{0}$
\end_inset

 can't be 1 if mean or variance of 
\begin_inset Formula $G\neq0$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
We can borrow strength from the ensemble of tests to bound 
\begin_inset Formula $\pi_{0}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame

\size scriptsize
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bounding 
\begin_inset Formula $\pi_{0}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
At any point 
\begin_inset Formula $p$
\end_inset

, density of 
\begin_inset Formula $p$
\end_inset

-values is mixure of true nulls (uniform) and false nulls (something else):
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $f_{p}(p)=\pi_{0}+(1-\pi_{0})f_{1}(p)$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Since 
\begin_inset Formula $f_{1}(p)\geq0$
\end_inset

, we have 
\begin_inset Formula $\pi_{0}\leq f_{p}(p)$
\end_inset

 for any 
\begin_inset Formula $p$
\end_inset

, so minimum density of 
\begin_inset Formula $p$
\end_inset

-values bounds 
\begin_inset Formula $\pi_{0}$
\end_inset

 (Efron et al., 2001):
\end_layout

\begin_layout Standard
\align center

\size scriptsize
\begin_inset Formula $\pi_{0}\leq{\displaystyle \min_{p}}\ f_{p}(p)$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Simple approach (Storey, 2002): calculate 
\begin_inset Formula $\hat{\pi}_{0}$
\end_inset

 as average density of 
\begin_inset Formula $p$
\end_inset

-values above threshold 
\begin_inset Formula $\lambda$
\end_inset

, beyond which we expect few false nulls
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Formally, 
\begin_inset Formula $\hat{\pi}_{0}=\dfrac{\sum_{j=1}^{J}1\{p_{j}>\lambda\}p_{j}}{(1-\lambda)J}$
\end_inset


\size footnotesize

\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\size scriptsize
Higher 
\begin_inset Formula $\lambda$
\end_inset

 means tighter bound but noisier estimate; Storey et al.
 (2004) discuss approaches to choosing 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Formula $q$
\end_inset

-values for FDR Control
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Given estimated bound 
\begin_inset Formula $\hat{\pi}_{0}$
\end_inset

, control 
\begin_inset Formula $FDR$
\end_inset

 using 
\series bold

\begin_inset Formula $\mathbf{q}$
\end_inset

-values
\series default
 (Storey, 2003):
\end_layout

\begin_layout Standard
\align center

\size footnotesize
\begin_inset Formula $q_{j}=\widehat{FDR}(p_{j})=\dfrac{p_{j}\hat{\pi}_{0}}{\hat{F}_{p}(p_{j})}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
\begin_inset Formula $q$
\end_inset

-value 
\begin_inset Formula $\approx$
\end_inset

 EB equivalent of 
\begin_inset Formula $p$
\end_inset

-value
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
Rather than controlling 
\begin_inset Formula $\Pr[R_{j}=1|T_{j}=1]$
\end_inset

, use Bayes rule + ensemble of tests to flip the conditioning and control
 
\begin_inset Formula $\Pr[T_{j}=1|R_{j}=1]$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pause
\end_layout

\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\size footnotesize
If unit 
\begin_inset Formula $j$
\end_inset

's 
\begin_inset Formula $q$
\end_inset

-val is 
\begin_inset Formula $q_{j}$
\end_inset

 and we reject all hypotheses with 
\begin_inset Formula $p$
\end_inset

-vals lower than 
\begin_inset Formula $p_{j}$
\end_inset

, we should expect at most 
\begin_inset Formula $100q_{j}\%$
\end_inset

 of rejections to be mistakes
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Formula $P$
\end_inset

-values from One-tailed Tests of No Discrimination
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename pvals_1.png
	lyxscale 50
	scale 45

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Formula $\hat{\pi}_{0}=0.31\implies$
\end_inset

 At Least 69% of Firms Discriminate
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename pvals_2.png
	lyxscale 50
	scale 45

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
31 Firms with 
\begin_inset Formula $q$
\end_inset

-vals Below 0.05
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename pvals_3.png
	lyxscale 50
	scale 45

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Industries and 
\begin_inset Formula $q$
\end_inset

-values from KRW (2022)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename qval_table.pdf
	lyxscale 50
	scale 55

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
References
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Abadie, A., and Kasy, M.
 (2019).
 
\begin_inset Quotes eld
\end_inset

Choosing among regularized estimators in empirical economics: the risk of
 machine learning.
\begin_inset Quotes erd
\end_inset

 
\emph on
Review of Economics and Statistics 
\emph default
101(5).
\end_layout

\begin_layout Itemize

\size scriptsize
Abowd, J., Kramarz, F., and Margolis, D.
 (1999).
 
\begin_inset Quotes eld
\end_inset

High-wage workers and high-wage firms.
\begin_inset Quotes erd
\end_inset

 
\emph on
Econometrica 
\emph default
67(2).
\end_layout

\begin_layout Itemize

\size scriptsize
Angrist, J., Hull, P., Pathak, P., and Walters, C.
 (2017).
 
\begin_inset Quotes eld
\end_inset

Leveraging lotteries for school value-added: testing and estimation.
\begin_inset Quotes erd
\end_inset

 
\emph on
Quarterly Journal of Economics
\emph default
 132(2).
\end_layout

\begin_layout Itemize

\size scriptsize
Angrist, J., Hull, P., Pathak, P., and Walters, C.
 (forthcoming).
 
\begin_inset Quotes eld
\end_inset

Simple and credible value-added estimation using centralized school assignment.
\begin_inset Quotes erd
\end_inset

 
\emph on
Review of Economics and Statistics.
\end_layout

\begin_layout Itemize

\size scriptsize
Anscombe, F.
 (1948).
 
\begin_inset Quotes eld
\end_inset

The transformation of poisson, binomial, and negative-binomial data.
\begin_inset Quotes erd
\end_inset

 
\emph on
Biometrika 
\emph default
35(3/4).
\end_layout

\begin_layout Itemize

\size scriptsize
Bartlett, M.
 (1947).
 
\begin_inset Quotes eld
\end_inset

The use of transformations.
\begin_inset Quotes erd
\end_inset

 
\emph on
Biometrics 
\emph default
3(1).
\end_layout

\begin_layout Itemize

\size scriptsize
Benjamini, Y., and Hochberg, Y.
 (1995).
 
\begin_inset Quotes eld
\end_inset

Controlling the false discovery rate: a practical and powerful approach
 to multiple testing.
\begin_inset Quotes erd
\end_inset

 
\emph on
Journal of the Royal Statistical Society: Series B
\emph default
 
\emph on
(Statistical Methodology)
\emph default
 57(1).
\end_layout

\begin_layout Itemize

\size scriptsize
Bertrand, M., and Mullainathan, S.
 (2004).
 
\begin_inset Quotes eld
\end_inset

Are Emily and Greg more employable than Lakisha and Jamal? A field experiment
 on labor market discrimination.
\begin_inset Quotes erd
\end_inset

 
\emph on
American Economic Review
\emph default
 94(4).
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
References
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Brown, L.
 (2008).
 
\begin_inset Quotes eld
\end_inset

In-season prediction of batting averages: a field test of empirical Bayes
 and Bayes methodologies.
\begin_inset Quotes erd
\end_inset

 
\emph on
The Annals of Applied Statistics 
\emph default
2(1).
\end_layout

\begin_layout Itemize

\size scriptsize
Card, D.
 (1999).
 
\begin_inset Quotes eld
\end_inset

The causal effect of education on earnings.
\begin_inset Quotes erd
\end_inset

 
\emph on
Handbook of Labor Economics
\emph default
 Volume 3.
\end_layout

\begin_layout Itemize

\size scriptsize
Card., D., Cardoso, A., Heining, J., and Kline, P.
 (2018).
 
\begin_inset Quotes eld
\end_inset

Firms and labor market inequality: evidence and some theory.
\begin_inset Quotes erd
\end_inset

 
\emph on
Journal of Labor Economics 
\emph default
36(S1).
\end_layout

\begin_layout Itemize

\size scriptsize
Chan, D., Gentzkow, M., and Yu, C.
 (2022).
 
\begin_inset Quotes eld
\end_inset

Selection with variation in diagnostic skill: evidence from radiologists.
\begin_inset Quotes erd
\end_inset

 
\emph on
Quarterly Journal of Economics 
\emph default
137(2).
\end_layout

\begin_layout Itemize

\size scriptsize
Chen, J.
 (2023).
 
\begin_inset Quotes eld
\end_inset

Empirical Bayes when estimation precision predicts parameters.
\begin_inset Quotes erd
\end_inset

 Working paper.
\end_layout

\begin_layout Itemize

\size scriptsize
Chetty, R., and Hendren, N.
 (2018).
 
\begin_inset Quotes eld
\end_inset

The impacts of neighborhoods on intergenerational mobility II: county-level
 estimates.
\begin_inset Quotes erd
\end_inset

 
\emph on
Quarterly Journal of Economics 
\emph default
133(3).Chetty, R., Friedman, J., Hendren, N., Jones, M., and Porter, S.
 (2018).
 
\begin_inset Quotes eld
\end_inset

The opportunity atlas: mapping the childhood roots of social mobility.
\begin_inset Quotes erd
\end_inset

 NBER working paper no.
 25147.
\end_layout

\begin_layout Itemize

\size scriptsize
Chetty, R., Friedman, J., and Rockoff, J.
 (2014).
 
\begin_inset Quotes eld
\end_inset

Measuring the impacts of teachers I: evaluating bias in teacher value-added
 estimates.
\begin_inset Quotes erd
\end_inset

 
\emph on
American Economic Review 
\emph default
104(9).
\end_layout

\begin_layout Itemize

\size scriptsize
Dale, S., and Krueger, A.
 (2002).
 
\begin_inset Quotes eld
\end_inset

Estimating the payoff to attending a more selective college: an application
 of selection on observables and unobservables.
\begin_inset Quotes erd
\end_inset

 
\emph on
Quarterly Journal of Economics
\emph default
 117(4).
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
References
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Dale, S., and Krueger, A.
 (2014).
 
\begin_inset Quotes eld
\end_inset

Estimating the effects of college characteristics over the career using
 administrative earnings data.
\begin_inset Quotes erd
\end_inset

 
\emph on
Journal of Human Resources
\emph default
 49(2).
\end_layout

\begin_layout Itemize

\size scriptsize
Efron, B., and Morris, C.
 (1973).
 
\begin_inset Quotes eld
\end_inset

Stein's estimation rule and its competitors – an empirical Bayes approach.
\begin_inset Quotes erd
\end_inset

 
\emph on
Journal of the American Statistical Association 
\emph default
68(341).
\end_layout

\begin_layout Itemize

\size scriptsize
Einav., L., Finkelstein, A., and Mahoney, N.
 (2022).
 
\begin_inset Quotes eld
\end_inset

Producing health: measuring value added of nursing homes.
\begin_inset Quotes erd
\end_inset

 NBER working paper no.
 30228.
\end_layout

\begin_layout Itemize

\size scriptsize
Efron, B.
 (2010).
 
\begin_inset Quotes eld
\end_inset

The future of indirect evidence.
\begin_inset Quotes erd
\end_inset

 
\emph on
Statistical Science 
\emph default
25(2).
\end_layout

\begin_layout Itemize

\size scriptsize
Efron, B.
 (2012).
 
\bar under
Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing,
 and Prediction
\bar default
.
 Cambridge University Press.
\end_layout

\begin_layout Itemize

\size scriptsize
Efron, B.
 (2016).
 
\begin_inset Quotes eld
\end_inset

Empirical Bayes deconvolution estimates.
\begin_inset Quotes erd
\end_inset

 
\emph on
Biometrika 
\emph default
103(1).
\end_layout

\begin_layout Itemize

\size scriptsize
Efron, B., Tibshirani, R., Storey, J., and Tusher, V.
 (2001).
 
\begin_inset Quotes eld
\end_inset

Empirical Bayes analysis of a microarray experiment.
\begin_inset Quotes erd
\end_inset

 
\emph on
Journal of the American Statistical Association 
\emph default
96(456).
\end_layout

\begin_layout Itemize

\size scriptsize
Fenizia, A.
 (2022).
 
\begin_inset Quotes eld
\end_inset

Managers and productivity in the public sector.
\begin_inset Quotes erd
\end_inset

 
\emph on
Econometrica 
\emph default
90(3).
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
References
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Frandsen, B., Lefgren, L., and Leslie, E.
 (2023).
 
\begin_inset Quotes eld
\end_inset

Judging judge fixed effects.
\begin_inset Quotes erd
\end_inset

 
\emph on
American Economic Review 
\emph default
113(1).
\end_layout

\begin_layout Itemize

\size scriptsize
Goncalves, F., and Mello, S.
 (2021).
 
\begin_inset Quotes eld
\end_inset

A few bad apples? Racial bias in policing.
\begin_inset Quotes erd
\end_inset

 
\emph on
American Economic Review 
\emph default
111(5).
\end_layout

\begin_layout Itemize

\size scriptsize
Gu, J., and Koenker, R.
 (2023).
 
\begin_inset Quotes eld
\end_inset

Invidious comparisons: ranking and selection as compound decisions.
\begin_inset Quotes erd
\end_inset

 
\emph on
Econometrica 
\emph default
91(1).
\end_layout

\begin_layout Itemize

\size scriptsize
Heckman, J., and Singer, B.
 (1984).
 
\begin_inset Quotes eld
\end_inset

A method for minimizing the impact of distributional assumptions in econometric
 models for duration data.
\begin_inset Quotes erd
\end_inset

 
\emph on
Econometrica 
\emph default
52(2).
\end_layout

\begin_layout Itemize

\size scriptsize
Holland, P.
 (1973).
 
\begin_inset Quotes eld
\end_inset

Covariance stabilizing transformations.
\begin_inset Quotes erd
\end_inset

 
\emph on
Annals of Statistics 
\emph default
1(1).
\end_layout

\begin_layout Itemize

\size scriptsize
James, W., and Stein, C.
 (1961).
 
\begin_inset Quotes eld
\end_inset

Estimation with quadratic loss.
\begin_inset Quotes erd
\end_inset

 
\emph on
Berkeley Symposium on Mathematical Statistics and Probability
\emph default
 1.
\end_layout

\begin_layout Itemize

\size scriptsize
Kiefer, J., and Wolfowitz, J.
 (1956).
 
\begin_inset Quotes eld
\end_inset

Consistency of the maximum likelihood estimator in the presence of infinitely
 many incidental parameters.
\begin_inset Quotes erd
\end_inset

 
\emph on
Annals of Mathematical Statistics
\emph default
 27(4).
\end_layout

\begin_layout Itemize

\size scriptsize
Kline, P., Rose, E., and Walters, C.
 (2022).
 
\begin_inset Quotes eld
\end_inset

Systemic discrimination among large US employers.
\begin_inset Quotes erd
\end_inset

 
\emph on
Quarterly Journal of Economics 
\emph default
137(4).
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
References
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Kline, P., Saggio, R., and Sølvsten, M.
 (2020).
 
\begin_inset Quotes eld
\end_inset

Leave-out estimation of variance components.
\begin_inset Quotes erd
\end_inset

 
\emph on
Econometrica 
\emph default
88(5).
\end_layout

\begin_layout Itemize

\size scriptsize
Kling, J., Liebman, J., and Katz, L.
 (2007).
 
\begin_inset Quotes eld
\end_inset

Experimental analysis of neighborhood effects.
\begin_inset Quotes erd
\end_inset

 
\emph on
Econometrica 
\emph default
75(1).
\end_layout

\begin_layout Itemize

\size scriptsize
Koenker, R., and Gu, J.
 (2017).
 
\begin_inset Quotes eld
\end_inset

REBayes: an R package for empirical Bayes mixture methods.
\begin_inset Quotes erd
\end_inset

 
\emph on
Journal of Statistical Software 
\emph default
82(8).
\end_layout

\begin_layout Itemize

\size scriptsize
Koenker, R., and Mizera, I.
 (2014).
 
\begin_inset Quotes eld
\end_inset

Convex optimization, shape constraints, compound decisions, and empirical
 Bayes rules.
\begin_inset Quotes erd
\end_inset

 
\emph on
Journal of the American Statistical Association 
\emph default
109(506).
\end_layout

\begin_layout Itemize

\size scriptsize
Krueger, A., and Summers, L.
 (1988).
 
\begin_inset Quotes eld
\end_inset

Efficiency wages and the inter-industry wage structure.
\begin_inset Quotes erd
\end_inset

 
\emph on
Econometrica 
\emph default
56(2).
\end_layout

\begin_layout Itemize

\size scriptsize
Morris, C.
 (1983).
 
\begin_inset Quotes eld
\end_inset

Parametric empirical Bayes inference: theory and applications.
\begin_inset Quotes erd
\end_inset

 
\emph on
Journal of the American Statistical Association 
\emph default
78(381).
\end_layout

\begin_layout Itemize

\size scriptsize
Mountjoy, J., and Hickman, B.
 (2021).
 
\begin_inset Quotes eld
\end_inset

The returns to college(s): relative value-added and match effects in higher
 education.
\begin_inset Quotes erd
\end_inset

 NBER working paper no.
 29276.
\end_layout

\begin_layout Itemize

\size scriptsize
Narasimhan, B., and Efron, B.
 (2020).
 
\begin_inset Quotes eld
\end_inset

deconvolveR: a G-modeling program for deconvolution and empirical Bayes
 estimation.
\begin_inset Quotes erd
\end_inset

 
\emph on
Journal of Statistical Software 
\emph default
94(11).
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
References
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size scriptsize
Raudenbush, S., and Bryk, A.
 (1986).
 
\begin_inset Quotes eld
\end_inset

A hierarchical model for studying school effects.
\begin_inset Quotes erd
\end_inset

 
\emph on
Sociology of Education 
\emph default
59(1).
\end_layout

\begin_layout Itemize

\size scriptsize
Robbins, H.
 (1950).
 
\begin_inset Quotes eld
\end_inset

A generalization of the method of maximum likelihood: estimating a mixing
 distribution.
\begin_inset Quotes erd
\end_inset

 
\emph on
Annals of Mathematical Statistics 
\emph default
21(2).
\end_layout

\begin_layout Itemize

\size scriptsize
Storey, J.
 (2002).
 
\begin_inset Quotes eld
\end_inset

A direct approach to false discovery rates.
\begin_inset Quotes erd
\end_inset

 
\emph on
Journal of the Royal Statistical Society: Series B
\emph default
 
\emph on
(Statistical Methodology)
\emph default
 64(3).
\end_layout

\begin_layout Itemize

\size scriptsize
Storey, J.
 (2003).
 
\begin_inset Quotes eld
\end_inset

The positive false discovery rate: a Bayesian interpretation and the 
\emph on
q
\emph default
-value.
\begin_inset Quotes erd
\end_inset

 
\emph on
Annals of Statistics 
\emph default
31(6).
\end_layout

\begin_layout Itemize

\size scriptsize
Storey, J., Taylor, J., and Siegmund, D.
 (2004).
 
\begin_inset Quotes eld
\end_inset

Strong control, conservative point estimation and simultaneous conservative
 consistency of false discovery rates: a unified approach.
\begin_inset Quotes erd
\end_inset

 
\emph on
Journal of the Royal Statistical Society: Series B
\emph default
 
\emph on
(Statistical Methodology)
\emph default
 66(1).
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\end_body
\end_document
